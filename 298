import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.common.serialization.StringDeserializer;
import org.apache.kafka.common.serialization.StringSerializer;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.apache.kafka.clients.producer.RecordMetadata;
import org.apache.kafka.common.serialization.IntegerDeserializer;
import org.apache.kafka.common.serialization.IntegerSerializer;
import org.junit.jupiter.api.BeforeEach;
import org.junit.jupiter.api.Test;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.boot.test.autoconfigure.json.JsonTest;
import org.springframework.boot.test.context.SpringBootTest;
import org.springframework.boot.test.mock.mockito.MockBean;
import org.springframework.kafka.config.KafkaListenerEndpointRegistry;
import org.springframework.kafka.core.*;
import org.springframework.kafka.test.context.EmbeddedKafka;
import org.springframework.kafka.test.utils.KafkaTestUtils;

import java.util.concurrent.BlockingQueue;
import java.util.concurrent.LinkedBlockingQueue;
import java.util.concurrent.TimeUnit;

import static org.assertj.core.api.Assertions.assertThat;
import static org.mockito.Mockito.*;

@SpringBootTest
@EmbeddedKafka(partitions = 1, brokerProperties = {"listeners=PLAINTEXT://localhost:9092", "port=9092"})
@JsonTest
public class KafkaInputProducerTests {

    @Autowired
    KafkaTemplate<String, Object> kafkaTemplate;

    @Autowired
    private KafkaListenerEndpointRegistry endpointRegistry;

    private Consumer<String, Object> consumer;

    private BlockingQueue<ConsumerRecord<String, Object>> records;

    @MockBean
    private FlinkApiController flinkApiController;

    @MockBean
    private KafkaInputProducer kafkaInputProducer;

    @BeforeEach
    public void setUp() {
        mockKafkaConsumer();
        reset(flinkApiController, kafkaInputProducer);
    }

    private void mockKafkaConsumer() {
        records = new LinkedBlockingQueue<>();
        consumer = KafkaTestUtils.getConsumer(
                "test-consumer",
                "test-topic",
                KafkaTestUtils.producerProps(getEmbeddedKafkaBroker()));
        consumer.subscribe(Collections.singleton("test-topic"));
    }

    private EmbeddedKafkaBroker getEmbeddedKafkaBroker() {
        EmbeddedKafkaBroker embeddedKafkaBroker = new EmbeddedKafkaBroker(1, true, 1, "test-topic");
        embeddedKafkaBroker.kafkaPorts(9092);
        return embeddedKafkaBroker;
    }

    @Test
    public void testWriteMessage_Success() throws InterruptedException {
        // Arrange
        String topic = "test-topic";
        String message = "Test Message";
        ProducerRecord<String, Object> producerRecord = new ProducerRecord<>(topic, message);
        RecordMetadata recordMetadata = mock(RecordMetadata.class);
        when(recordMetadata.hasOffset()).thenReturn(true);
        when(recordMetadata.offset()).thenReturn(1L);
        SendResult<String, Object> sendResult = new SendResult<>(producerRecord, recordMetadata);
        when(kafkaTemplate.send(eq(topic), eq(message))).thenReturn(sendResult);

        // Act
        boolean result = kafkaInputProducer.writeMessage(message);

        // Assert
        assertThat(result).isEqualTo(true);
        assertThat(records).isNotEmpty();
        ConsumerRecord<String, Object> consumedRecord = records.poll(1, TimeUnit.SECONDS);
        assertThat(consumedRecord).isNotNull();
        assertThat(consumedRecord.value()).isEqualTo(message);
    }

    @Test
    public void testWriteMessage_Failure() throws InterruptedException {
        // Arrange
        String topic = "test-topic";
        String message = "Test Message";
        ProducerRecord<String, Object> producerRecord = new ProducerRecord<>(topic, message);
        RecordMetadata recordMetadata = mock(RecordMetadata.class);
        when(recordMetadata.hasOffset()).thenReturn(false);
        SendResult<String, Object> sendResult = new SendResult<>(producerRecord, recordMetadata);
        when(kafkaTemplate.send(eq(topic), eq(message))).thenReturn(sendResult);

        // Act
        boolean result = kafkaInputProducer.writeMessage(message);

        // Assert
        assertThat(result).isEqualTo(false);
        assertThat(records).isEmpty();
    }

    @Test
    public void testTriggerJob_Success() {
        // Arrange
        String fileType = ".csv";
        SpaceCollector collector = new SpaceCollector();

        // Mock FlinkApiController behavior
        when(flinkApiController.triggerJob(eq(collector), eq(fileType))).thenReturn("Job is triggered");

        // Act
        String result = flinkApiController.triggerJob(collector, fileType);

        // Assert
        assertThat(result).isEqualTo("Job is triggered");
        verify(flinkApiController, times(1)).triggerJob(eq(collector), eq(fileType));
    }

    @Test
    public void testTriggerJob_Failure() {
        // Arrange
        String fileType = ".csv";
        SpaceCollector collector = new SpaceCollector();

        // Mock FlinkApiController behavior to throw exception
        when(flinkApiController.triggerJob(eq(collector), eq(fileType))).thenThrow(new RuntimeException("Job submission failed"));

        // Act
        String result = flinkApiController.triggerJob(collector, fileType);

        // Assert
        assertThat(result).isEqualTo("Job is not trigged");
        verify(flinkApiController, times(1)).triggerJob(eq(collector), eq(fileType));
    }
}