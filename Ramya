We have our flink cluster a standalone entity on to which we upload our jar files and then we perform the transformation by calling our apis from data transformer client. Now what we are going to do is instead of uploading it as a jar and then performing the transformer, calling the transformation from data transformer client.what we will be doing is we will not have this flink cluster standalone all 3 will dissappear now our application it self will be deployed inside it and it will spin up a small flink cluster so there will be two applications here one is xml and another one is csv just like our rest of the applications there are two types of changes that has to be done here one has to do with the code level where we have to start consuming from our kafka topic .
We are already using stream execution environment where our source in here is a file we are taking it up as a file and when we get all the parameters then we start process and it works.
One at a time now here we have to make a change where our source will be kafka topic .This other topic will be, will not get these parameters like this, but we'll be consuming from our audit topic from there we will be consuming and we will get all necessary informations from there itself. Including everything 
Yes, from this order topic only we have to consume and trigger the flink job i mean skip all the messages from other messages like if you get a collection failure or if you get flink job unsuccessful we will have to skip and just trigger flink job successul .
Whenever we encounter audit message stating the collection/jobstatus successful  we will have to continue this process