To implement AC1 and AC2, you can create a method in the FlinkApiController class called "updateJobStatus" that takes the auditMsg as a parameter. 

Here is the updated code:

FlinkApiController.java:

```java
@Service
public class FlinkApiController {

    @Value("${flink.api.url}")
    private String flinkApiUrl;

    @Value("${flink.job.csv.jarid}")
    private String flinkJobJarid;

    @Value("${flink.job.csv.program-args}")
    private String programArgs;

    @Value("${flink.job.txt.jarid}")
    private String flinkJobtxtJarid;

    @Value("${flink.job.xml.program-args}")
    private String programXmlArgs;

    @Value("${flink.job.xml.jarid}")
    private String flinkJobXmlJarid;

    @Value("${log.directory.path}")
    private String logFolderPath;

    private KafkaConsumer KafkaConsumer = new KafkaConsumer();
    @Autowired
    private KafkaAuditProducer kafkaAuditProducer;
    HttpHeaders headers = new HttpHeaders();

    private static final Logger LOGGER = LoggerFactory.getLogger(FlinkApiController.class);

    @PostMapping
    public String triggerJob(UnifiedAuditMessage auditMsg, String fileType) {

        RestTemplate restTemplate = new RestTemplate();
        LOGGER.info("TRIGGER JOB::::");
        HttpEntity<String> request = null;
        try {
            String jobSubmitUrl = null;
            if (programArgs != null && !programArgs.isEmpty()) {
                headers.setContentType(MediaType.APPLICATION_JSON);

                JSONObject requestBody = new JSONObject();
                try {
                    if (fileType.equalsIgnoreCase(".csv")) {
                        requestBody.put("programArgs", programArgs.replace("input", auditMsg.getInputFilePath()));
                        jobSubmitUrl = flinkApiUrl + "/jars/" + flinkJobJarid + "/run";

                    } else if (fileType.equalsIgnoreCase(".xml")) {
                        LOGGER.info("XML FLINK TRIGGER::");

                        programXmlArgs = programXmlArgs.replace("|filePath|", auditMsg.getInputFilePath());
                        programXmlArgs = programXmlArgs.replace(" |primaryKey|", "");
                        requestBody.put("programArgs", programXmlArgs);
                        jobSubmitUrl = flinkApiUrl + "/jars/" + flinkJobXmlJarid + "/run";
                        LOGGER.info("jobSubmitUrl:" + jobSubmitUrl);
                        LOGGER.info("requestBody::" + requestBody.toString());

                    } else if (fileType.equalsIgnoreCase(".txt")) {
                        LOGGER.info("TEXT FLINK TRIGGER::");

                        requestBody.put("programArgs", programArgs.replace("|input|", auditMsg.getInputFilePath()));

                        jobSubmitUrl = flinkApiUrl + "/jars/" + flinkJobtxtJarid + "/run";

                    }

                    request = new HttpEntity<String>(requestBody.toString(), headers);
                }

                // Trigger the submitted jar
                ResponseEntity<String> response = restTemplate.postForEntity(jobSubmitUrl, request, String.class);
                LOGGER.info("response: {}", response);

                if (response != null && response.getStatusCode() == HttpStatus.OK) {
                    updateJobStatus(auditMsg);
                    return "Job submitted";
                } else {
                    LOGGER.info("Not Ok!!");
                    updateJobStatus(auditMsg);
                    kafkaAuditProducer.createMessageAndLog(auditMsg.getInputFilePath(), auditMsg.getUrl(),
                            auditMsg.getPort(), auditMsg.getOutputFilePath(), auditMsg.getDelimiters(), fileType,
                            new SimpleDateFormat("yyyy-MM-dd HH:mm:ss").format(new Date()),
                            auditMsg.getNo_of_records_in_the_file(), auditMsg.getNo_of_records_processed(),
                            auditMsg.getNo_of_records_failed(), auditMsg.getExceptions(), auditMsg.getJob_id(),
                            auditMsg.getJar_id(), auditMsg.getCollector_id());
                    KafkaConsumer.writeLogToFile(auditMsg, fileType, response, logFolderPath);
                }
            }
        } catch (Exception e) {
            e.printStackTrace();
            LOGGER.error("Error triggering job", e.getMessage());
            kafkaAuditProducer.createMessageAndLog(auditMsg.getInputFilePath(), auditMsg.getUrl(), auditMsg.getPort(),
                    auditMsg.getOutputFilePath(), auditMsg.getDelimiters(), fileType,
                    new SimpleDateFormat("yyyy-MM-dd HH:mm:ss").format(new Date()),
                    auditMsg.getNo_of_records_in_the_file(), auditMsg.getNo_of_records_processed(),
                    auditMsg.getNo_of_records_failed(), auditMsg.getJob_id(), auditMsg.getJar_id(),
                    auditMsg.getCollector_id(), auditMsg.getExceptions());
            KafkaConsumer.writeLogToFile(auditMsg, fileType, logFolderPath);
        }
        return "Job is not triggered";
    }

    private void updateJobStatus(UnifiedAuditMessage auditMsg) {
        try {
            RestTemplate restTemplate = new RestTemplate();
            JsonNode jsonNode = restTemplate.getForObject(flinkApiUrl + jobID, JsonNode.class);

            while (true) {
                if (jsonNode.get("state").textValue().equals("FINISHED") ||
                        jsonNode.get("state").textValue().equals("CANCELED") ||
                        jsonNode.get("state").textValue().equals("FAILED") ||
                        jsonNode.get("state").textValue().equals("SUSPENDED")) {
                    break;
                }
                Thread.sleep(100);
            }

            if (jsonNode.get("state").textValue().equals("FINISHED")) {
                auditMsg.setJob_status(JobStatus.FLINK_JOB_SUBMITTED.toString());
            } else {
                auditMsg.setJob_status(JobStatus.FLINK_JOB_FAILED.toString());
            }
        } catch (Exception e) {
            LOGGER.error("Error updating job status", e.getMessage());
        }
    }
}
```

Hope this helps!