Here is the updated code that writes the failure message to a log file in the destinationFolderPath shared location:

KafkaProducer.java:

import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.kafka.core.KafkaTemplate;
import org.springframework.stereotype.Service;
import org.apache.logging.log4j.LogManager;
import org.apache.logging.log4j.Logger;

import java.io.FileWriter;
import java.io.IOException;
import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.time.LocalDateTime;
import java.time.format.DateTimeFormatter;

@Service
public class KafkaProducer {

    private static final Logger logger = LogManager.getLogger(KafkaProducer.class);

    @Value("${spring.kafka.producer.topic-name}")
    private String topicName;

    private final KafkaTemplate<String, String> kafkaTemplate;

    @Autowired
    public KafkaProducer(KafkaTemplate<String, String> kafkaTemplate) {
        this.kafkaTemplate = kafkaTemplate;
    }

    public String sendMessage(String message) {
        logger.info(String.format("Message sent %s", message));
        kafkaTemplate.send(topicName, message);
        return "Message sent successfully";
    }

    public void sendErrorMessage(FileMetadata fileMetadata, String destinationFolderPath) {
        try {
            String errorMessage = JsonUtils.convertFileMetadataToJson(fileMetadata);
            sendMessage(errorMessage);
            writeToLogFile(destinationFolderPath, errorMessage);
        } catch (JsonProcessingException e) {
            logger.error("Error sending error message to Kafka: {}", e.getMessage());
        }
    }
    
    private void writeToLogFile(String destinationFolderPath, String errorMessage) {
        try {
            String logFileName = getLogFileName();
            Path logFilePath = Paths.get(destinationFolderPath, logFileName);
            Files.writeString(logFilePath, errorMessage);
            logger.info("Error message written to log file: {}", logFilePath.toString());
        } catch (IOException e) {
            logger.error("Error writing error message to log file: {}", e.getMessage());
        }
    }
    
    private String getLogFileName() {
        LocalDateTime now = LocalDateTime.now();
        DateTimeFormatter formatter = DateTimeFormatter.ofPattern("yyyyMMdd_HHmmss");
        return "error_" + now.format(formatter) + ".log";
    }
}

KafkaConsumer.java:

import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.kafka.annotation.KafkaListener;
import org.springframework.stereotype.Service;
import org.apache.logging.log4j.LogManager;
import org.apache.logging.log4j.Logger;

import java.io.BufferedReader;
import java.io.FileReader;
import java.io.IOException;
import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.time.LocalDateTime;
import java.time.format.DateTimeFormatter;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.List;

@Service
public class KafkaConsumer {

    private static final Logger logger = LogManager.getLogger(KafkaConsumer.class);

    @Value("${file.txtsource}")
    private String sourceFilePath;

    @Value("${file.destination}")
    private String destinationFolderPath;

    @Autowired
    private FlinkApiController flinkApi;

    @Autowired
    private KafkaProducer kafkaProducer;

    @KafkaListener(topics = "${spring.kafka.producer.topic-name}", groupId = "${spring.kafka.consumer.group-id}")
    public void consume(String data) {
        try {
            SpaceCollector message = JsonUtils.convertJsonToSpaceCollector(data);
            logger.info("Message received: {}", message);

            String sourceFilePath = message.getInputFilePath();

            String response = null;

            if (validateFile(sourceFilePath)) {
                writeFile(sourceFilePath, destinationFolderPath);

                String fileType = getExtension(sourceFilePath);

                if (fileType != null && fileType.equalsIgnoreCase(".xml")) {
                    logger.info("XML FILE");
                    retrieveXmlFile(sourceFilePath);
                } else if (fileType != null && fileType.equalsIgnoreCase(".csv")) {
                    logger.info("CSV FILE");
                    retrieveCsvFile(sourceFilePath);
                } else if (fileType != null && fileType.equalsIgnoreCase(".txt")) {
                    logger.info("TEXT FILE");
                    retrieveTxtFile(sourceFilePath);
                }

                response = flinkApi.triggerJob(message, fileType);
            } else {
                FileMetadata fileMetadata = new FileMetadata();
                fileMetadata.setFile_name(sourceFilePath);
                fileMetadata.setJob_status("Failed");
                fileMetadata.setExceptions("File does not exist: " + sourceFilePath);
                kafkaProducer.sendErrorMessage(fileMetadata, destinationFolderPath);
                logger.error("File does not exist: {}", sourceFilePath);
            }

            logger.info("Flink Response: {}", response);
        } catch (JsonProcessingException e) {
            logger.error("Error processing JSON message: {}", e.getMessage());
        }
    }

    private void writeFile(String sourceFilePath, String destinationFolderPath) throws IOException {
        Path sourcePath = Paths.get(sourceFilePath);
        Path destinationPath = Paths.get(destinationFolderPath, sourcePath.getFileName().toString());
        Files.copy(sourcePath, destinationPath, StandardCopyOption.REPLACE_EXISTING);
    }

    private boolean validateFile(String sourceFilePath) {
        File file = new File(sourceFilePath);
        return file.exists();
    }

    private List<String[]> retrieveCsvFile(String sourceFilePath) {
        List<String[]> rows = new ArrayList<>();
        try (CSVParser parser = new CSVParser(new FileReader(sourceFilePath), CSVFormat.DEFAULT)) {
            for (CSVRecord record : parser) {
                String[] row = new String[record.size()];
                for (int i = 0; i < record.size(); i++) {
                    row[i] = record.get(i);
                }
                rows.add(row);
                logger.info("CSV row: {}", Arrays.toString(row));
            }
            logger.info("Successfully read {} rows from CSV file {}", rows.size(), sourceFilePath);
        } catch (IOException e) {
            logger.error("Error reading CSV file: {}", sourceFilePath, e);
        }
        return rows;
    }
}

In KafkaConsumer.java, a new method `writeToLogFile` is added in the KafkaProducer class to write the error message to a log file in the destinationFolderPath shared location. The method `sendErrorMessage` in KafkaProducer.java is updated to call the new method and pass the destinationFolderPath as a parameter. The `writeToLogFile` method uses the `Files.writeString` method to write the error message to a file. The log file name is generated with the current timestamp using the `getLogFileName` method.