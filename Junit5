UnifiedAuditMessage class:-
```java
@Data
@NoArgsConstructor
@AllArgsConstructor
public class UnifiedAuditMessage {
    @NotBlank
    private String inputFilePath;
    @NotBlank
    private String url;
    @NotNull
    private Integer port;
    @NotBlank
    private String outputFilePath;
    @NotBlank
    private String delimiters;
    @NotBlank
    private String fileType;
    @NotNull
    private String dateReceived;
    private String dateProcessed;
    @NotNull
    private Integer noOfRecordsInTheFile;
    @NotNull
    private Integer noOfRecordsProcessed;
    @NotNull
    private Integer noOfRecordsFailed;
    @NotNull
    private String jobId;
    @NotNull
    private String jarId;
    private List<String> exceptions;
    @NotNull
    private String collectorId;
    @NotNull
    private JobStatus jobStatus;
}
```

KafkaAuditProducer.java:-
```java
@Service
public class KafkaAuditProducer {
    private static final Logger Logger = LoggerFactory.getLogger(KafkaAuditProducer.class);

    @Value("${spring.kafka.producer.topic-name}")
    private String topicName;

    private final KafkaTemplate<String, Object> kafkaTemplate;

    @Autowired
    public KafkaAuditProducer(KafkaTemplate<String, Object> kafkaTemplate) {
        this.kafkaTemplate = kafkaTemplate;
    }

    public String sendMessage(UnifiedAuditMessage auditMsg) {
        Logger.info(String.format("Message sent %s", auditMsg));
        kafkaTemplate.send(topicName, auditMsg);
        return "Message sent successfully";
    }

    public void createMessageAndLog(UnifiedAuditMessage auditMsg, JobStatus jobStatus) {
        LocalDateTime dateProcessed = LocalDateTime.now();
        String dateProcessedStr = dateProcessed.format(DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss.SSS"));
        auditMsg.setDateProcessed(dateProcessedStr);
        auditMsg.setJobStatus(jobStatus);

        try {
            ObjectMapper objectMapper = new ObjectMapper();
            String auditMsgJson = objectMapper.writeValueAsString(auditMsg); 
            this.kafkaTemplate.send(topicName, auditMsgJson);
            Logger.info("Kafka status message {}", auditMsgJson);
        } catch (JsonProcessingException e) {
            Logger.error("Failed to convert auditMsg to JSON: {}", e.getMessage());
        } catch (Exception e) { 
            Logger.error("Failed to send Kafka message: {}", e.getMessage());
        }
    }
}
```

FlinkApiController.java:-
```java
@Service
public class FlinkApiController {

    @Value("${flink.api.url}")
    private String flinkApiUrl;

    @Value("${flink.job.csv.jarid}")
    private String flinkJobJarid;

    @Value("${flink.job.csv.program-args}")
    private String programArgs;

    @Value("${flink.job.txt.jarid}")
    private String flinkJobtxtJarid;

    @Value("${flink.job.xml.program-args}")
    private String programXmlArgs;

    @Value("${flink.job.xml.jarid}")
    private String flinkJobXmlJarid;

    @Value("${log.directory.path}")
    private String logFolderPath;

    private KafkaConsumer KafkaConsumer = new KafkaConsumer();
    
    private final KafkaAuditProducer kafkaAuditProducer;
    
    private final Utils utils;

    private static final Logger LOGGER = LoggerFactory.getLogger(FlinkApiController.class);

    @Autowired
    public FlinkApiController(KafkaAuditProducer kafkaAuditProducer, Utils utils) {
        this.kafkaAuditProducer = kafkaAuditProducer;
        this.utils = utils;
    }

    @PostMapping
    public String triggerJob(UnifiedAuditMessage auditMsg, String fileType) {

        RestTemplate restTemplate = new RestTemplate();
        LOGGER.info("TRIGGER JOB::::");
        HttpHeaders headers = new HttpHeaders();
        HttpEntity<String> request = null;
        String jobSubmitUrl = null;

        if (programArgs != null && !programArgs.isEmpty()) {
            headers.setContentType(MediaType.APPLICATION_JSON);

            JSONObject requestBody = new JSONObject();
            try {
                if (fileType.equalsIgnoreCase(".csv")) {
                    requestBody.put("programArgs", programArgs.replace("input", auditMsg.getInputFilePath()));
                    jobSubmitUrl = flinkApiUrl + "/jars/" + flinkJobJarid + "/run";
                } else if (fileType.equalsIgnoreCase(".xml")) {
                    LOGGER.info("XML FLINK TRIGGER::");
                    programXmlArgs = programXmlArgs.replace("|filePath|", auditMsg.getInputFilePath());
                    programXmlArgs = programXmlArgs.replace(" |primaryKey|", "");
                    requestBody.put("programArgs", programXmlArgs);
                    jobSubmitUrl = flinkApiUrl + "/jars/" + flinkJobXmlJarid + "/run";
                    LOGGER.info("jobSubmitUrl:" + jobSubmitUrl);
                    LOGGER.info("requestBody::" + requestBody.toString());
                } else if (fileType.equalsIgnoreCase(".txt")) {
                    LOGGER.info("TEXT FLINK TRIGGER::");
                    requestBody.put("programArgs", programArgs.replace("|input|", auditMsg.getInputFilePath()));
                    jobSubmitUrl = flinkApiUrl + "/jars/" + flinkJobtxtJarid + "/run";
                }

                request = new HttpEntity<>(requestBody.toString(), headers);
            } catch (JSONException e) {
                LOGGER.error("Error creating JSON request body: {}", e.getMessage());
                kafkaAuditProducer.createMessageAndLog(auditMsg, JobStatus.FLINK_JOB_FAILED);
                return "Job is not triggered";
            }
        }

        // Trigger the submitted jar
        ResponseEntity<String> response = restTemplate.postForEntity(jobSubmitUrl, request, String.class);
        LOGGER.info("response: {}", response);

        if (response != null && response.getStatusCode() == HttpStatus.OK) {
            kafkaAuditProducer.createMessageAndLog(auditMsg, JobStatus.FLINK_JOB_SUBMITTED);
            return "Job submitted";
        } else {
            LOGGER.info("Not OK!!");
            UnifiedAuditMessage auditData = KafkaConsumer.logMetadata(auditMsg, fileType, response);
            kafkaAuditProducer.createMessageAndLog(auditMsg, JobStatus.FLINK_JOB_FAILED);
            kafkaConsumer.writeLogToFile(auditMsg, fileType, response, logFolderPath);
        }
        
        return "Job is not triggered";
    }
}
```

KafkaConsumer.java class:- 
```java
@Service
public class KafkaConsumer {
    private static final Logger LOGGER LoggerFactory.getLogger(KafkaConsumer.class);
    
    @Value("${file.txtsource}")
    private String sourceFilePath;
    
    @Value("${file.destination}")
    private String destinationFolderPath;
    
    @Value("${spring.kafka.producer.topic-name}")
    private String topic;
    
    private final KafkaTopicUtil util;
    
    private final FlinkApiController flinkApi;

    private final KafkaAuditProducer kafkaAuditProducer;

    @Autowired
    public KafkaConsumer(KafkaTopicUtil util, FlinkApiController flinkApi, KafkaAuditProducer kafkaAuditProducer) {
        this.util = util;
        this.flinkApi = flinkApi;
        this.kafkaAuditProducer = kafkaAuditProducer;
    }

    @KafkaListener(topics = "${spring.kafka.producer.topic-name}", groupId = "${spring.kafka.consumer.group-id}",
            errorHandler = "kafkaEventErrorHandler")
    public void consume(String data) throws IOException {
        LocalDateTime dateReceived = LocalDateTime.now();
        UnifiedAuditMessage auditMsg = null;
        String fileType = null