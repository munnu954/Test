Now yours is the two part thing see understand first you will have to check inside the lucine, inside space collectors .Whether we are collecting it or not, ok? If one record exists for this, that means we are collecting, right?This particular host, then you will know that yes, this is the last time we collected this data and last time we successfully transformed this data. Okay, so there will be two dates.Whether we are collecting it or not, ok? If one record exists for this, that means we are collecting, right?Now you need to provide details about what was the,when was the last collection? When did the last collection happen So how you will do that? You'll search with the same post Inside our order engine. Inside the audit table collection underscore audit This particular host, then you will know that yes, this is the last time we collected this data and last time we successfully transformed this data.
You needed to add a new model and these 2 values have to be fetched from COLLECTION_AUDIT data as per status
COLLECTION_SUCCESSFUL and FLINK _JOB_SUCCESSFUL
this model will only be used in your API so no need to add in SPACE_COLLECTORS
Expected input:
URL: checkStatus?host=localhost

Expected output:
{
"shPublicKey":
"gsh-key-01",
"userName"; "uname",
"type": "CSV
GIGAMON",
"connectionType": "SSH",
"url": "localhost",
"enabled": "false",
"outputFilePath": "/space-shared-storage/destination/",
"password":
"",
"pollingDuration": "2024-01-01T01:00:00.000",
"port "：22，
"sshPrivateKey": "ssh-key-01",
"delimiter":
", ",
"id": "1",
"clusterState": "WAITING",
"inputFilePath": "/home/inputdir",
"lastAliveTime": "2024-01-01T00:00:25.000",
"fileType": "CSV GIGAMON"
"lastSuccessfulCollection": "2024-05-2500:00:25.000" //To be extracted
"lastSuccessfulTansformation": "2024-05-25T00:00:50.000" / /To be
from COLLECTION AUDIT for the same extracted Erom COLLECTION AUDIT fOr the
EE
host
with status=COLLECTION
_SUCCESSFUL
same host with status=FLINK JOB_ SUCCESSFUL
}