import org.apache.flink.api.common.JobID;
import org.apache.flink.api.java.utils.ParameterTool;
import org.apache.flink.configuration.Configuration;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.util.StringUtils;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.util.Properties;

public class CsTransformerJob {
    private static final Logger LOGGER = LoggerFactory.getLogger(CsTransformerJob.class);
    private static final PublishAuditMessage pubAuditMsg = new PublishAuditMessage();
    private static final AppProperties appProp = AppProperties.getInstance();
    private static final ClassicHttpClient classicHttpClient = new ClassicHttpClient();
    private static final String flinkJobName = "Csv to JSON Transformer Job";

    public void execute() throws Exception {
        String bootstrapServer = appProp.getAppProperties("spring-kafka.consumer.bootstrap-servers");
        String auditTopic = appProp.getAppProperties("spring-kafka.audit-topic-name");
        String groupId = appProp.getAppProperties("spring.kafka.consumer-group-id");
        String offsets = appProp.getAppProperties("spring-kafka.consumer.auto-offset-reset");

        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        
        try {
            CollectionAudit collectionAudit = getCollectionAudit(bootstrapServer);

            try {
                CsvDataTransformerUtil.validateKafka(collectionAudit.getBootstrapServer(),
                        collectionAudit.getOutputFilePath(), auditTopic);
            } catch (Exception e) {
                LOGGER.error("Error in CsvTransformerApplication main connecting to Kafka or topic not available ()", e);
            }

            LOGGER.info("bootstrapServer {}, auditTopic {}, groupId {}, offsets {}", bootstrapServer, auditTopic,
                    groupId, offsets);

            DataStream<CollectionAudit> collectionAuditStream = getCollectionAuditStream(env, bootstrapServer, auditTopic, groupId, offsets, collectionAudit);
            DataStream<Tuple2<String, CollectionAudit>> csvDataStream = getCsvDataStream(collectionAuditStream);
            DataStream<Tuple2<JSONObject, CollectionAudit>> jsonNodeStream = getJsonNodeStream(csvDataStream, collectionAudit.getFileType());
            DataStream<Tuple2<String, CollectionAudit>> finalDataStream = getFinalDataStream(jsonNodeStream);

            if (finalDataStream != null) {
                publishTransformedData(finalDataStream, null);
            }

            JobClient jobClient = env.executeAsync(flinkJobName);
            JobID jobId = jobClient.getJobID();
            LOGGER.info("JobID is {}", jobId);
        } catch (Exception e) {
            LOGGER.error("Error while executing CsvTransformerJob {}", e);
        }
    }

    private static CollectionAudit getCollectionAudit(String bootstrapServer) {
        CollectionAudit collectionAudit = new CollectionAudit();
        collectionAudit.setOutputFilePath(appProp.getAppProperties("spring.kafka.data-topic-name"));
        collectionAudit.setFlinkURL(appProp.getAppProperties("flink.api.url"));
        collectionAudit.setBootstrapServer(bootstrapServer);
        return collectionAudit;
    }

    // Other methods (getCollectionAuditStream, getCsvDataStream, getJsonNodeStream, getFinalDataStream, publishTransformedData) are to be implemented
}
