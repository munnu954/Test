Sure! Here's an implementation of the JUnit5 test cases for AC1 and AC2:

```java
import static org.mockito.ArgumentMatchers.*;
import static org.mockito.Mockito.*;

import java.time.LocalDateTime;
import java.util.Arrays;
import java.util.Collections;
import java.util.List;

import org.junit.jupiter.api.Test;
import org.mockito.ArgumentCaptor;
import org.mockito.Captor;
import org.mockito.InjectMocks;
import org.mockito.Mock;
import org.mockito.MockitoAnnotations;
import org.springframework.http.HttpEntity;
import org.springframework.http.HttpHeaders;
import org.springframework.http.HttpStatus;
import org.springframework.http.MediaType;
import org.springframework.http.ResponseEntity;
import org.springframework.web.client.RestTemplate;

class FlinkApiControllerTest {

    @Mock
    private RestTemplate restTemplate;

    @Mock
    private KafkaAuditProducer kafkaAuditProducer;

    @InjectMocks
    private FlinkApiController flinkApiController;

    @Captor
    private ArgumentCaptor<HttpEntity<String>> requestCaptor;

    public FlinkApiControllerTest() {
        MockitoAnnotations.initMocks(this);
    }

    @Test
    void testTriggerJob_SuccessfulSubmission() throws Exception {
        // given
        UnifiedAuditMessage auditMsg = createUnifiedAuditMessage();
        ResponseEntity<String> response = new ResponseEntity<>(createResponseJson("job-id"), HttpStatus.OK);

        when(restTemplate.postForEntity(anyString(), any(HttpEntity.class), eq(String.class))).thenReturn(response);

        // when
        flinkApiController.triggerJob(auditMsg, ".csv");

        // then
        verify(kafkaAuditProducer).createMessageAndLog(
                eq(auditMsg.getInputFilePath()), eq(auditMsg.getUrl()), eq(auditMsg.getPort()),
                eq(auditMsg.getOutputFilePath()), eq(auditMsg.getDelimiters()), eq(".csv"), anyString(),
                eq(auditMsg.getNo_of_records_in_the_file()), eq(auditMsg.getNo_of_records_processed()),
                eq(auditMsg.getNo_of_records_failed()), eq(auditMsg.getJob_id()), eq(auditMsg.getJar_id()),
                eq(auditMsg.getCollector_id()), eq(auditMsg.getExceptions()), eq("FLINK_JOB_SUBMITTED")
        );

        verify(restTemplate).postForEntity(anyString(), requestCaptor.capture(), eq(String.class));

        HttpEntity<String> request = requestCaptor.getValue();
        HttpHeaders headers = request.getHeaders();
        String requestBody = request.getBody();

        assertThat(headers.getContentType()).isEqualTo(MediaType.APPLICATION_JSON);
        assertThat(requestBody).contains("programArgs", "input", auditMsg.getInputFilePath());
    }

    @Test
    void testTriggerJob_UnsuccessfulSubmission() throws Exception {
        // given
        UnifiedAuditMessage auditMsg = createUnifiedAuditMessage();
        ResponseEntity<String> response = new ResponseEntity<>(createResponseJson("job-id"), HttpStatus.BAD_REQUEST);

        when(restTemplate.postForEntity(anyString(), any(HttpEntity.class), eq(String.class))).thenReturn(response);

        // when
        flinkApiController.triggerJob(auditMsg, ".csv");

        // then
        verify(kafkaAuditProducer).createMessageAndLog(
                eq(auditMsg.getInputFilePath()), eq(auditMsg.getUrl()), eq(auditMsg.getPort()),
                eq(auditMsg.getOutputFilePath()), eq(auditMsg.getDelimiters()), eq(".csv"), anyString(),
                eq(auditMsg.getNo_of_records_in_the_file()), eq(auditMsg.getNo_of_records_processed()),
                eq(auditMsg.getNo_of_records_failed()), eq(auditMsg.getJob_id()), eq(auditMsg.getJar_id()),
                eq(auditMsg.getCollector_id()), eq(auditMsg.getExceptions())
        );

        verify(restTemplate).postForEntity(anyString(), requestCaptor.capture(), eq(String.class));
        verify(kafkaAuditProducer).writeLogToFile(eq(auditMsg), eq(".csv"), eq(response), eq(logFolderPath));

        HttpEntity<String> request = requestCaptor.getValue();
        HttpHeaders headers = request.getHeaders();
        String requestBody = request.getBody();

        assertThat(headers.getContentType()).isEqualTo(MediaType.APPLICATION_JSON);
        assertThat(requestBody).contains("programArgs", "input", auditMsg.getInputFilePath());
    }

    private UnifiedAuditMessage createUnifiedAuditMessage() {
        LocalDateTime now = LocalDateTime.now();

        return new UnifiedAuditMessage(
                "inputFilePath", "collectorId", "url", 12345, "outputFilePath", "delimiters",
                ".csv", "dateReceived", now.toString(), 10, 5, 2,
                Collections.singletonList("exception1"), "jobId", "jarId", "jobStatus"
        );
    }

    private String createResponseJson(String jobId) {
        return "{\"jobid\":\"" + jobId + "\",\"state\":\"RUNNING\"}";
    }
}
```

Note: In the test cases, you need to set the relevant values for the `UnifiedAuditMessage` object and the `logFolderPath` variable according to your requirements. Also, make sure to import required libraries and classes for the test cases to compile and run successfully.



@Test
@DisplayName("Test transformation job submission success")
void testTransformationJobSubmissionSuccess() throws Exception {
    // Arrange
    String inputFilePath = "/path/to/input/file";
    String url = "http://example.com";
    int port = 8080;
    String outputFilePath = "/path/to/output/file";
    String delimiters = ",";
    String fileType = ".csv";
    String dateReceived = "2022-01-01 10:10:10";
    int noOfRecordsInTheFile = 100;
    int noOfRecordsProcessed = 100;
    int noOfRecordsFailed = 0;
    String jobId = "12345";
    String jarId = "67890";
    String collectorId = "abcde";
    List<String> exceptions = Collections.emptyList();
    String jobStatus = JobStatus.FLINK_JOB_SUBMITTED.toString();
    UnifiedAuditMessage auditMsg = new UnifiedAuditMessage(inputFilePath, collectorId, url, port, outputFilePath, delimiters, fileType, dateReceived, noOfRecordsInTheFile, noOfRecordsProcessed, noOfRecordsFailed, exceptions, jobId, jarId, jobStatus);

    KafkaTemplate<String, UnifiedAuditMessage> kafkaTemplate = mock(KafkaTemplate.class);
    KafkaAuditProducer kafkaAuditProducer = new KafkaAuditProducer(kafkaTemplate);

    when(kafkaTemplate.send(eq(topicName), any(ProducerRecord.class))).thenAnswer(invocation -> {
        ProducerRecord<String, UnifiedAuditMessage> record = invocation.getArgument(1);
        assertEquals(topicName, record.topic());
        assertEquals(auditMsg, record.value());
        return null;
    });

    // Act
    kafkaAuditProducer.createMessageAndLog(auditMsg.getInputFilePath(), auditMsg.getUrl(), auditMsg.getPort(), auditMsg.getOutputFilePath(), auditMsg.getDelimiters(), auditMsg.getFileType(), auditMsg.getDate_received(), auditMsg.getNo_of_records_in_the_file(), auditMsg.getNo_of_records_processed(), auditMsg.getNo_of_records_failed(), auditMsg.getJob_id(), auditMsg.getJar_id(), auditMsg.getCollector_id(), auditMsg.getExceptions(), auditMsg.getJobStatus());

    // Assert
    verify(kafkaTemplate, times(1)).send(eq(topicName), any(ProducerRecord.class));
}