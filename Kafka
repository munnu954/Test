To fix the error "Error while deserializing Json: java.lang.NullPointerException: Cannot invoke...", you need to check if the value of the `data` variable is null before deserializing it. Also, ensure that the `inputFilePath` field in the received JSON is not null.

Here's the updated code for the KafkaConsumer class with the fixed error:

```java
@Service
public class KafkaConsumer {
    private static final Logger LOGGER = LoggerFactory.getLogger(KafkaConsumer.class);
  
    @Value("${file.txtsource}")
    private String sourceFilePath;
  
    @Value("${file.destination}")
    private String destinationFolderPath;
  
    @Value("${spring.kafka.producer.topic-name}")
    private String topic;
  
    @Autowired
    private KafkaTopicUtil util;

    @Autowired
    private FlinkApiController FlinkApi;

    @Autowired
    private KafkaAuditProducer kafkaAuditProducer;

    @KafkaListener(topics="${spring.kafka.producer.topic-name}", groupId = "${spring.kafka.consumer.group-id}", errorHandler = "kafkaEventErrorHandler")
    public void consume(String data) throws IOException {
        LocalDateTime dataReceived = LocalDateTime.now();
        UnifiedAuditMessage auditMsg = null;
        String fileType = null;
      
        try {
            if (data != null && data.contains("inputFilePath")) {
                ObjectMapper mapper = new ObjectMapper();
                auditMsg = mapper.readValue(data, UnifiedAuditMessage.class);
              
                if (auditMsg != null) {
                    LOGGER.info("Message Received: {}", auditMsg);
                    LOGGER.info("Message received: {}", auditMsg);
                    String sourceFilePath = auditMsg.getInputFilePath();
                    String response = null;

                    if ((new File(sourceFilePath).exists()) && !util.isKafkaTopicPresent(topic)) {
                        UnifiedAuditMessage auditData = new UnifiedAuditMessage();
                        auditData.setFile_name(sourceFilePath);
                        auditData.setExceptions("Input file path doesn't exist/Invalid file");
                        writeLogToFile(auditData, destinationFolderPath);
                    }

                    if (validateFile(sourceFilePath)) {
                        writeFile(sourceFilePath, destinationFolderPath);
                        String fileType = getExtension(sourceFilePath);

                        if (fileType != null && fileType.equalsIgnoreCase(".xml")) {
                            LOGGER.info("XML FILE");
                            retrieveXmlFile(sourceFilePath);
                        } else if (fileType != null && fileType.equalsIgnoreCase(".csv")) {
                            LOGGER.info("CSV FILE");
                            retrieveCsvFile(sourceFilePath);
                        } else if (fileType != null && fileType.equalsIgnoreCase(".txt")) {
                            LOGGER.info("TEXT FILE");
                            retrieveTxtFile(sourceFilePath);
                        }

                        LOGGER.info("Flink Response: {}", response);
                    } else {
                        kafkaAuditProducer.sendErrorMessage("File does not exist");
                    }
                }
            }
        } catch (Exception e) {
            e.printStackTrace();
            LOGGER.error("Error triggering job", e.getMessage());
            UnifiedAuditMessage auditData = logAuditData(auditMsg, fileType, e);
            kafkaAuditProducer.createMessageAndLog(auditData.getInputFilePath(), auditData.getUrl(), auditData.getPort(), auditData.getOutputFilePath(), auditData.getDelimiters(), auditData.getFileType(), auditData.getDate_received(), auditData.getNo_of_records_in_the_file(), auditData.getNo_of_records_processed(), auditData.getNo_of_records_failed(), auditData.getJob_id(), auditData.getJar_id(), auditData.getExceptions(), auditData.getCollector_id(), auditData.getJob_status());
        }
    }

    public UnifiedAuditMessage logAuditData(UnifiedAuditMessage auditMsg, String fileType, Exception e) {
        DateTimeFormatter formatter = DateTimeFormatter.ofPattern("yyyy-MM-dd'T'HH:mm:ss");
        LocalDateTime currentDate = LocalDateTime.now();

        return new UnifiedAuditMessage(
            auditMsg.getInputFilePath(),
            auditMsg.getUrl(),
            auditMsg.getPort(),
            auditMsg.getOutputFilePath(),
            auditMsg.getDelimiters(),
            fileType,
            currentDate,
            currentDate,
            auditMsg.getNo_of_records_in_the_file(),
            auditMsg.getNo_of_records_processed(),
            auditMsg.getNo_of_records_failed(),
            UUID.randomUUID(),
            UUID.randomUUID(),
            Collections.singletonList(e.getMessage()),
            auditMsg.getCollector_id(),
            UnifiedAuditMessage.JobStatus.COLLECTION_FAILED
        );
    }
}
```

And here's the updated code for the KafkaAuditProducer class:

```java
@Service
public class KafkaAuditProducer {
    private static final Logger Logger = LoggerFactory.getLogger(KafkaAuditProducer.class);

    @Value("${spring.kafka.producer.topic-name}")
    private String topicName;

    private final KafkaTemplate<String, Object> kafkaTemplate;

    @Autowired
    public KafkaAuditProducer(KafkaTemplate<String, Object> kafkaTemplate) {
        this.kafkaTemplate = kafkaTemplate;
    }

    public void createMessageAndLog(String inputFilePath, String url, Integer port, String outputFilePath, String delimiters, String fileType, LocalDateTime dateReceived, Integer no_of_records_in_the_file, Integer no_of_records_processed, Integer no_of_records_failed, UUID job_id, UUID jar_id, List<String> exceptions, UUID collector_id, UnifiedAuditMessage.JobStatus job_status) {
        LocalDateTime dateProcessed = LocalDateTime.now();
        DateTimeFormatter formatter = DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss.SSS");
        String date_processed = dateProcessed.format(formatter);

        UnifiedAuditMessage auditMsg = new UnifiedAuditMessage(inputFilePath, url, port, outputFilePath, delimiters, fileType, dateReceived, date_processed, no_of_records_in_the_file, no_of_records_processed, no_of_records_failed, job_id, jar_id, exceptions, collector_id, job_status);

        try {
            ObjectMapper objectMapper = new ObjectMapper();
            String auditMsgJson = objectMapper.writeValueAsString(auditMsg);
            this.kafkaTemplate.send(topicName, auditMsgJson);
            Logger.info("Kafka status message: {}", auditMsgJson);
        } catch (JsonProcessingException e) {
            Logger.error("Failed to convert auditMsg to JSON: {}", e.getMessage());
        } catch (Exception e) {
            Logger.error("Failed to send Kafka message: {}", e.getMessage());
        }
    }
  
    public boolean writeMessage(UnifiedAuditMessage auditMsg) {
        try {
            this.kafkaTemplate.send(topicName, auditMsg);
            Logger.info("Kafka status message {}", auditMsg);
        } catch (Exception e) {
            return false;
        }
        return true;
    }
}
```

Note that I've also fixed some minor syntax errors and typos in the code. Make sure to replace the old code with the updated one and retest it.