SpaceCollector.java model class: 

import lombok.AllArgsConstructor;
import lombok.Data;
import lombok.NoArgsConstructor;
import lombok.ToString;

@Data
@NoArgsConstructor
@AllArgsConstructor
@ToString
public class SpaceCollector {
    private Integer id;
    private String url;
    @Max(value = 9999, message = "Only 4 digit port number allowed")
    private Integer port;
    private String inputFilePath;
    private String outputFilePath;
    private String delimiters;
    private String fileType;
}

FileMetadata.java model class:

import lombok.AllArgsConstructor;
import lombok.Data;
import lombok.NoArgsConstructor;
import lombok.ToString;

@Data
@NoArgsConstructor
@AllArgsConstructor
@ToString
public class FileMetadata {
    private String file_name;
    private Date date_received;
    private Date date_processed;
    private Integer no_of_records_in_the_file;
    private Integer no_of_records_processed;
    private Integer no_of_records_failed;
    private String job_status;
    private String job_id;
    private String jar_id;
    private String exceptions;
}

JsonUtils.java class:

import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.databind.ObjectMapper;

public class JsonUtils {
    public static String convertSpaceCollectorToJson(SpaceCollector sp) throws JsonProcessingException {
        return new ObjectMapper().writeValueAsString(sp);
    }

    public static SpaceCollector convertJsonToSpaceCollector(String data) throws JsonProcessingException {
        return new ObjectMapper().readValue(data, SpaceCollector.class);
    }

    public static String convertFileMetadataToJson(FileMetadata fileMetadata) throws JsonProcessingException {
        return new ObjectMapper().writeValueAsString(fileMetadata);
    }

    public static FileMetadata convertJsonToFileMetadata(String data) throws JsonProcessingException {
        return new ObjectMapper().readValue(data, FileMetadata.class);
    }
}

KafkaProducer.java class:

import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.kafka.core.KafkaTemplate;
import org.springframework.stereotype.Service;
import org.apache.logging.log4j.LogManager;
import org.apache.logging.log4j.Logger;

@Service
public class KafkaProducer {

    private static final Logger logger = LogManager.getLogger(KafkaProducer.class);

    @Value("${spring.kafka.producer.topic-name}")
    private String topicName;

    private final KafkaTemplate<String, String> kafkaTemplate;

    @Autowired
    public KafkaProducer(KafkaTemplate<String, String> kafkaTemplate) {
        this.kafkaTemplate = kafkaTemplate;
    }

    public String sendMessage(String message) {
        logger.info(String.format("Message sent %s", message));
        kafkaTemplate.send(topicName, message);
        return "Message sent successfully";
    }

    public void sendErrorMessage(FileMetadata fileMetadata) {
        try {
            String errorMessage = JsonUtils.convertFileMetadataToJson(fileMetadata);
            sendMessage(errorMessage);
        } catch (JsonProcessingException e) {
            logger.error("Error sending error message to Kafka: {}", e.getMessage());
        }
    }
}

KafkaController.java:

import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.web.bind.annotation.PostMapping;
import org.springframework.web.bind.annotation.RequestBody;
import org.springframework.web.bind.annotation.RestController;

@RestController
public class KafkaController {
    @Autowired
    private KafkaProducer kafkaProducer;

    @PostMapping("/kafkaPush")
    public Boolean sendMessage(@RequestBody SpaceCollector message) {
        kafkaProducer.sendMessage(message);
        return true;
    }
}

FlinkApiController.java:

import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.http.HttpEntity;
import org.springframework.http.HttpHeaders;
import org.springframework.http.MediaType;
import org.springframework.stereotype.Service;
import org.springframework.web.client.RestTemplate;

@Service
public class FlinkApiController {

    @Value("${flink.api.url}")
    private String flinkApiUrl;

    @Value("${flink.job.csv.jarid}")
    private String flinkJobJarid;

    @Value("${flink.job.csv.program-args}")
    private String programArgs;

    @Value("${flink.job.txt.jarid}")
    private String flinkJobtxtJarid;

    @Value("${flink.job.xml.program-args}")
    private String programXmlArgs;

    @Value("${flink.job.xml.jarid}")
    private String flinkJobXmlJarid;

    private static final Logger logger = LogManager.getLogger(FlinkApiController.class);

    private final HttpHeaders headers = new HttpHeaders();

    @Autowired
    private RestTemplate restTemplate;

    public String triggerJob(SpaceCollector collector, String fileType) {
        logger.info("TRIGGER JOB::::");
        HttpEntity<String> request;

        if (programArgs != null && !programArgs.isEmpty()) {
            headers.setContentType(MediaType.APPLICATION_JSON);

            JSONObject requestBody = new JSONObject();

            if (fileType.equalsIgnoreCase(".csv")) {
                requestBody.put("programArgs", programArgs.replace("input", collector.getInputFilePath()));
                String jobSubmitUrl = flinkApiUrl + "/jars/" + flinkJobJarid + "/run";
                request = new HttpEntity<>(requestBody.toString(), headers);
                return sendJobSubmitRequest(jobSubmitUrl, request);
            } else if (fileType.equalsIgnoreCase(".xml")) {
                logger.info("XML FLINK TRIGGER::");
                programXmlArgs = programXmlArgs.replace("|filePath|", collector.getInputFilePath());
                programXmlArgs = programXmlArgs.replace(" |primaryKey|", "");
                requestBody.put("programArgs", programXmlArgs);
                String jobSubmitUrl = flinkApiUrl + "/jars/" + flinkJobXmlJarid + "/run";
                request = new HttpEntity<>(requestBody.toString(), headers);
                return sendJobSubmitRequest(jobSubmitUrl, request);
            } else if (fileType.equalsIgnoreCase(".txt")) {
                logger.info("TEXT FLINK TRIGGER::");
                requestBody.put("programArgs", programArgs.replace("|input|", collector.getInputFilePath()));
                String jobSubmitUrl = flinkApiUrl + "/jars/" + flinkJobtxtJarid + "/run";
                request = new HttpEntity<>(requestBody.toString(), headers);
                return sendJobSubmitRequest(jobSubmitUrl, request);
            }
        }

        return null;
    }

    private String sendJobSubmitRequest(String jobSubmitUrl, HttpEntity<String> request) {
        try {
            String response = restTemplate.postForObject(jobSubmitUrl, request, String.class);
            logger.info("Response: {}", response);
            return response;
        } catch (Exception e) {
            logger.error("Error submitting job: {}", e.getMessage());
            return null;
        }
    }
}

KafkaConsumer.java class:

import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.kafka.annotation.KafkaListener;
import org.springframework.stereotype.Service;
import org.apache.logging.log4j.LogManager;
import org.apache.logging.log4j.Logger;

import java.io.BufferedReader;
import java.io.FileReader;
import java.io.IOException;
import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.List;

@Service
public class KafkaConsumer {

    private static final Logger logger = LogManager.getLogger(KafkaConsumer.class);

    @Value("${file.txtsource}")
    private String sourceFilePath;

    @Value("${file.destination}")
    private String destinationFolderPath;

    @Autowired
    private FlinkApiController flinkApi;

    @Autowired
    private KafkaProducer kafkaProducer;

    @KafkaListener(topics = "${spring.kafka.producer.topic-name}", groupId = "${spring.kafka.consumer.group-id}")
    public void consume(String data) throws IOException {
        try {
            SpaceCollector message = JsonUtils.convertJsonToSpaceCollector(data);
            logger.info("Message received: {}", message);

            String sourceFilePath = message.getInputFilePath();

            String response = null;

            if (validateFile(sourceFilePath)) {
                writeFile(sourceFilePath, destinationFolderPath);

                String fileType = getExtension(sourceFilePath);

                if (fileType != null && fileType.equalsIgnoreCase(".xml")) {
                    logger.info("XML FILE");
                    retrieveXmlFile(sourceFilePath);
                } else if (fileType != null && fileType.equalsIgnoreCase(".csv")) {
                    logger.info("CSV FILE");
                    retrieveCsvFile(sourceFilePath);
                } else if (fileType != null && fileType.equalsIgnoreCase(".txt")) {
                    logger.info("TEXT FILE");
                    retrieveTxtFile(sourceFilePath);
                }

                response = flinkApi.triggerJob(message, fileType);
            } else {
                kafkaProducer.sendErrorMessage("File does not exist: " + sourceFilePath);
                logger.error("File does not exist: {}", sourceFilePath);
            }

            logger.info("Flink Response: {}", response);
        } catch (JsonProcessingException e) {
            logger.error("Error processing JSON message: {}", e.getMessage());
        }
    }

    private void writeFile(String sourceFilePath, String destinationFolderPath) throws IOException {
        Path sourcePath = Paths.get(sourceFilePath);
        Path destinationPath = Paths.get(destinationFolderPath, sourcePath.getFileName().toString());
        Files.copy(sourcePath, destinationPath, StandardCopyOption.REPLACE_EXISTING);
    }

    private boolean validateFile(String sourceFilePath) {
        File file = new File(sourceFilePath);
        return file.exists();
    }

    private List<String[]> retrieveCsvFile(String sourceFilePath) {
        List<String[]> rows = new ArrayList<>();
        try (CSVParser parser = new CSVParser(new FileReader(sourceFilePath), CSVFormat.DEFAULT)) {
            for (CSVRecord record : parser) {
                String[] row = new String[record.size()];
                for (int i = 0; i < record.size(); i++) {
                    row[i] = record.get(i);
                }
                rows.add(row);
                logger.info("CSV row: {}", Arrays.toString(row));
            }
            logger.info("Successfully read {} rows from CSV file {}", rows.size(), sourceFilePath);
        } catch (IOException e) {
            logger.error("Error reading CSV file: {}", sourceFilePath, e);
        }
        return rows;
    }



----------------------++++---

Updated KafkaConsumer.java class:

import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.kafka.annotation.KafkaListener;
import org.springframework.stereotype.Service;
import org.apache.logging.log4j.LogManager;
import org.apache.logging.log4j.Logger;

import java.io.BufferedReader;
import java.io.FileReader;
import java.io.IOException;
import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.nio.file.StandardCopyOption;
import java.time.LocalDateTime;
import java.time.format.DateTimeFormatter;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.List;

@Service
public class KafkaConsumer {

    private static final Logger logger = LogManager.getLogger(KafkaConsumer.class);

    @Value("${file.txtsource}")
    private String sourceFilePath;

    @Value("${file.destination}")
    private String destinationFolderPath;

    @Autowired
    private FlinkApiController flinkApi;

    @Autowired
    private KafkaProducer kafkaProducer;

    @KafkaListener(topics = "${spring.kafka.producer.topic-name}", groupId = "${spring.kafka.consumer.group-id}")
    public void consume(String data) throws IOException {
        try {
            SpaceCollector message = JsonUtils.convertJsonToSpaceCollector(data);
            logger.info("Message received: {}", message);

            String sourceFilePath = message.getInputFilePath();
            String logFilePath = getLogFilePath(message);

            String response = null;

            if (validateFile(sourceFilePath)) {
                writeFile(sourceFilePath, destinationFolderPath);

                String fileType = getExtension(sourceFilePath);

                if (fileType != null && fileType.equalsIgnoreCase(".xml")) {
                    logger.info("XML FILE");
                    retrieveXmlFile(sourceFilePath);
                } else if (fileType != null && fileType.equalsIgnoreCase(".csv")) {
                    logger.info("CSV FILE");
                    retrieveCsvFile(sourceFilePath);
                } else if (fileType != null && fileType.equalsIgnoreCase(".txt")) {
                    logger.info("TEXT FILE");
                    retrieveTxtFile(sourceFilePath);
                }

                response = flinkApi.triggerJob(message, fileType);
            } else {
                String errorMessage = "File does not exist: " + sourceFilePath;
                kafkaProducer.sendErrorMessage(message);
                logger.error(errorMessage);
                writeLogToFile(errorMessage, logFilePath);
            }

            logger.info("Flink Response: {}", response);
        } catch (JsonProcessingException e) {
            logger.error("Error processing JSON message: {}", e.getMessage());
        }
    }

    private void writeFile(String sourceFilePath, String destinationFolderPath) throws IOException {
        Path sourcePath = Paths.get(sourceFilePath);
        Path destinationPath = Paths.get(destinationFolderPath, sourcePath.getFileName().toString());
        Files.copy(sourcePath, destinationPath, StandardCopyOption.REPLACE_EXISTING);
    }

    private boolean validateFile(String sourceFilePath) {
        File file = new File(sourceFilePath);
        return file.exists();
    }

    private List<String[]> retrieveCsvFile(String sourceFilePath) {
        List<String[]> rows = new ArrayList<>();
        try (CSVParser parser = new CSVParser(new FileReader(sourceFilePath), CSVFormat.DEFAULT)) {
            for (CSVRecord record : parser) {
                String[] row = new String[record.size()];
                for (int i = 0; i < record.size(); i++) {
                    row[i] = record.get(i);
                }
                rows.add(row);
                logger.info("CSV row: {}", Arrays.toString(row));
            }
            logger.info("Successfully read {} rows from CSV file {}", rows.size(), sourceFilePath);
        } catch (IOException e) {
            logger.error("Error reading CSV file: {}", sourceFilePath, e);
        }
        return rows;
    }
  
    private String getExtension(String sourceFilePath) {
        int dotIndex = sourceFilePath.lastIndexOf(".");
        if (dotIndex > 0) {
            return sourceFilePath.substring(dotIndex);
        }
        return null;
    }
  
    private String getLogFilePath(SpaceCollector message) {
        DateTimeFormatter formatter = DateTimeFormatter.ofPattern("yyyyMMddHHmmss");
        String currentDateTime = LocalDateTime.now().format(formatter);
        String logFileName = "log_" + currentDateTime + ".txt";
        return Paths.get(destinationFolderPath, logFileName).toString();
    }
  
    private void writeLogToFile(String errorMessage, String logFilePath) {
        try (BufferedWriter writer = new BufferedWriter(new FileWriter(logFilePath, true))) {
            writer.write(errorMessage);
            writer.newLine();
            writer.flush();
        } catch (IOException e) {
            logger.error("Error writing log to file: {}", logFilePath, e);
        }
    }
}
