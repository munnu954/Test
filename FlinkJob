Here is the modified code with the added functionality:

Main.java:

```java
import org.apache.flink.api.common.functions.FlatMapFunction;
import org.apache.flink.api.java.utils.ParameterTool;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.api.functions.sink.SinkFunction;
import org.apache.flink.streaming.connectors.kafka.KafkaRecordSerializationSchema;
import org.apache.flink.streaming.connectors.kafka.KafkaSink;
import org.apache.flink.streaming.connectors.kafka.internals.DeliveryGuarantee;
import org.apache.flink.streaming.connectors.kafka.internals.KafkaSerializationSchemaWrapper;
import org.apache.flink.streaming.util.serialization.SimpleStringSchema;
import org.apache.flink.util.Collector;
import org.json.JSONObject;

import java.io.File;
import java.io.FileNotFoundException;
import java.nio.file.Path;
import java.util.Arrays;
import java.util.List;

public class Main {

    private static String bootstrapServer;
    private static String topic;
    private static String[] headers;

    private static final Logger LOGGER = LoggerFactory.getLogger(Main.class);

    public static void main(String[] args) throws Exception {
        ParameterTool parameters = ParameterTool.fromArgs(args);

        bootstrapServer = parameters.get("bootstrapServer");
        topic = parameters.get("topic");

        LOGGER.info("Start to read and pick the path");

        String path = parameters.getRequired("input");
        if (!(new File(path).exists())) {
            throw new FileNotFoundException("File not found!");
        }

        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1); // Set parallelism to 1 for this example

        KafkaSink<String> sink = KafkaSink.<String>builder()
                .setBootstrapServers(bootstrapServer)
                .setRecordSerializer(KafkaRecordSerializationSchema.builder()
                        .setTopic(topic)
                        .setValueSerializationSchema(new SimpleStringSchema())
                        .build())
                .setDeliveryGuarantee(DeliveryGuarantee.AT_LEAST_ONCE)
                .build();

        final DataStream<String> stream = env.readTextFile(path)
                .flatMap(new CsvToJsonTransformer())
                .sinkTo(sink);

        env.execute("Flink Kafka Sink Example");
    }
}
```

CsvToJsonTransformer.java:
```java
import org.apache.flink.api.common.functions.FlatMapFunction;
import org.apache.flink.util.Collector;
import org.json.JSONObject;

public class CsvToJsonTransformer implements FlatMapFunction<String, String> {
    private String[] headers;

    @Override
    public void flatMap(String s, Collector<String> collector) throws Exception {
        String[] arr = s.split(",");

        if (headers == null) {
            headers = arr; // Store the header row for key names
            return; // Skip processing the header row
        }

        JSONObject obj = new JSONObject();

        for (int i = 0; i < arr.length; i++) {
            String jsonKey = headers[i].trim();
            String jsonValue = arr[i].trim().replace("\"", ""); // Remove any quotes around the value
            obj.put(jsonKey, jsonValue);
        }

        collector.collect(obj.toString());
    }
}
```

Note that I made a few changes to the original code for better readability and clarity:

- Removed unused imports.
- Added a separate variable for the headers array.
- Changed the Logger declaration to `private static final Logger LOGGER = LoggerFactory.getLogger(Main.class);`
- Set parallelism to 1 in the StreamExecutionEnvironment for this example.
- Added a name for the execution environment using `env.execute("Flink Kafka Sink Example");`.

Please make sure to import the required packages and adjust the code according to your specific use case.