To fix the error "com.fasterxml.jackson.databind.exc.MismatchedInputException: Cannot construct instance of org.vdsi.space.collections.customcsvdatatransformer.model.UnifiedAuditMessage (although at least one Creator exists): no String-argument constructor/factory method to deserialize from String value", you need to add a String-argument constructor to your `UnifiedAuditMessage` class.

Here's the updated code for the `UnifiedAuditMessage` class:

```java
@Data
@AllArgsConstructor
@NoArgsConstructor
public class UnifiedAuditMessage {
    // existing properties
    
    // Add String-argument constructor
    public UnifiedAuditMessage(String inputFilePath, String url, Integer port, String outputFilePath, String delimiters, String fileType,
                            String date_received, String date_processed, Integer no_of_records_in_the_file, Integer no_of_records_processed,
                            Integer no_of_records_failed, UUID job_id, UUID jar_id, List<String> exceptions, UUID collector_id, JobStatus job_status) {
        // Initialize properties
        this.inputFilePath = inputFilePath;
        this.url = url;
        this.port = port;
        this.outputFilePath = outputFilePath;
        this.delimiters = delimiters;
        this.fileType = fileType;
        this.date_received = date_received;
        this.date_processed = date_processed;
        this.no_of_records_in_the_file = no_of_records_in_the_file;
        this.no_of_records_processed = no_of_records_processed;
        this.no_of_records_failed = no_of_records_failed;
        this.job_id = job_id;
        this.jar_id = jar_id;
        this.exceptions = exceptions;
        this.collector_id = collector_id;
        this.job_status = job_status;
    }

    // existing enum and getters/setters
}
```

Now, let's update the remaining classes:

KafkaAuditProducer.java:
```java
@Service
public class KafkaAuditProducer {
    // existing code
    
    public void createMessageAndLog(UnifiedAuditMessage auditMsg) {
        LocalDateTime dateProcessed = LocalDateTime.now();
        String date_processed = dateProcessed.format(DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss.SSS"));
        
        UnifiedAuditMessage updatedAuditMsg = new UnifiedAuditMessage(
                auditMsg.getInputFilePath(), auditMsg.getUrl(), auditMsg.getPort(), auditMsg.getOutputFilePath(),
                auditMsg.getDelimiters(), auditMsg.getFileType(), auditMsg.getDate_received(), date_processed,
                auditMsg.getNo_of_records_in_the_file(), auditMsg.getNo_of_records_processed(),
                auditMsg.getNo_of_records_failed(), auditMsg.getJob_id(), auditMsg.getJar_id(),
                auditMsg.getExceptions(), auditMsg.getCollector_id(), auditMsg.getJob_status()
        );
        
        try {
            ObjectMapper objectMapper = new ObjectMapper();
            String auditMsgJson = objectMapper.writeValueAsString(updatedAuditMsg);
            
            this.kafkaTemplate.send(topicName, auditMsgJson);
            Logger.info("Kafka status message: {}", auditMsgJson);
        } catch (JsonProcessingException e) {
            Logger.error("Failed to convert auditMsg to JSON: {}", e.getMessage());
        } catch (Exception e) {
            Logger.error("Failed to send Kafka message: {}", e.getMessage());
        }
    }
    
    // existing code
}
```

KafkaController.java:
```java
@RestController
public class KafkaController {
    @Autowired
    private KafkaAuditProducer kafkaAuditProducer;

    @PostMapping("/kafkaPush")
    public boolean writeMessage(@RequestBody UnifiedAuditMessage auditMsg) {
        kafkaAuditProducer.createMessageAndLog(auditMsg);
        return true;
    }
}
```

FlinkApiController.java:
```java
@Service
public class FlinkApiController {
    // existing code
    
    public void triggerJob(UnifiedAuditMessage auditMsg, String fileType) {
        RestTemplate restTemplate = new RestTemplate();
        LOGGER.info("TRIGGER JOB::::");
        HttpHeaders headers = new HttpHeaders();
        headers.setContentType(MediaType.APPLICATION_JSON);
        
        JSONObject requestBody = new JSONObject();
        try {
            String programArgs = "";
            if (".csv".equalsIgnoreCase(fileType)) {
                programArgs = "input " + auditMsg.getInputFilePath()
                        + " --path " + auditMsg.getInputFilePath() + " -fileType csv_GIGAMON"
                        + " --bootstarpServer localhost:9092"
                        + " --auditTopic 3459-DEV-COLLECTION-AUDIT-QUEUE"
                        + " --outputTopic COLLECTION-OUTPUT";
                requestBody.put("programArgs", programArgs);
                String jobSubmitUrl = flinkApiUrl + "/jars/" + flinkJobJarid + "/run";
                request = new HttpEntity<>(requestBody.toString(), headers);
                ResponseEntity<String> response = restTemplate.postForEntity(jobSubmitUrl, request, String.class);
                LOGGER.info("Response: {}", response);
            } else if (".xml".equalsIgnoreCase(fileType)) {
                LOGGER.info("XML FLINK TRIGGER::");
                programArgs = "--filePath " + auditMsg.getInputFilePath()
                        + " --primaryKey |primaryKey|"
                        + " --bootstarpServer localhost:9092"
                        + " --auditTopic 3459-DEV-COLLECTION-AUDIT-QUEUE"
                        + " --dataTopic COLLECTION-TOPIC";
                requestBody.put("programArgs", programArgs);
                String jobSubmitUrl = flinkApiUrl + "/jars/" + flinkJobXmlJarid + "/run";
                request = new HttpEntity<>(requestBody.toString(), headers);
                ResponseEntity<String> response = restTemplate.postForEntity(jobSubmitUrl, request, String.class);
                LOGGER.info("Response: {}", response);
            } else if (".txt".equalsIgnoreCase(fileType)) {
                LOGGER.info("TEXT FLINK TRIGGER::");
                programArgs = "--input " + auditMsg.getInputFilePath()
                        + " --path " + auditMsg.getInputFilePath()
                        + " --bootstarpServer localhost:9092"
                        + " --auditTopic 3459-DEV-COLLECTION-AUDIT-QUEUE"
                        + " --dataTopic COLLECTION-TOPIC";
                requestBody.put("programArgs", programArgs);
                String jobSubmitUrl = flinkApiUrl + "/jars/" + flinkJobtxtJarid + "/run";
                request = new HttpEntity<>(requestBody.toString(), headers);
                ResponseEntity<String> response = restTemplate.postForEntity(jobSubmitUrl, request, String.class);
                LOGGER.info("Response: {}", response);
            }
        } catch (Exception e) {
            LOGGER.error("Error triggering job: {}", e.getMessage());
        }
    }
    
    // existing code
}
```

KafkaConsumer.java:
```java
@Service
public class KafkaConsumer {
    // existing code
    
    public void consume(String data) throws IOException {
        LocalDateTime dateReceived = LocalDateTime.now();
        UnifiedAuditMessage auditMsg = null;
        String fileType = null;
        
        try {
            if (data.contains("inputFilePath")) {
                ObjectMapper mapper = new ObjectMapper();
                auditMsg = mapper.readValue(data, UnifiedAuditMessage.class);
                LOGGER.info("Message Received: {}", auditMsg);
                LOGGER.info("Message received: {}", auditMsg);
                
                String sourceFilePath = auditMsg.getInputFilePath();
                String response = null;
                
                // ... existing code ...
                
                if (validateFile(sourceFilePath)) {
                    // ... existing code ...
                    
                    response = getFlinkResponse();
                    LOGGER.info("Flink Response: {}", response);
                } else {
                    kafkaAuditProducer.sendErrorMessage("File does not exist");
                }
            }
        } catch (Exception e) {
            e.printStackTrace();
            LOGGER.error("Error triggering job: {}", e.getMessage());
            
            UnifiedAuditMessage updatedAuditMsg = logAuditData(auditMsg, fileType, e);
            kafkaAuditProducer.createMessageAndLog(updatedAuditMsg);
        }
    }
    
    public UnifiedAuditMessage logAuditData(UnifiedAuditMessage auditMsg, String fileType, Exception e) {
        LocalDateTime currentDate = LocalDateTime.now();
      
        return new UnifiedAuditMessage(
                auditMsg.getInputFilePath(),
                auditMsg.getUrl(),
                auditMsg.getPort(),
                auditMsg.getOutputFilePath(),
                auditMsg.getDelimiters(),
                fileType,
                currentDate.toString(),
                currentDate.toString(),
                auditMsg.getNo_of_records_in_the_file(),
                auditMsg.getNo_of_records_processed(),
                auditMsg.getNo_of_records_failed(),
                UUID.randomUUID(),
                UUID.randomUUID(),
                Collections.singletonList(e.getMessage()),
                auditMsg.getCollector_id(),
                UnifiedAuditMessage.JobStatus.COLLECTION_FAILED
        );
    }
    
    // existing code
}
```

With these updates, the `UnifiedAuditMessage`