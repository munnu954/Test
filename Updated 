KafkaProducer.java: 

@Service
public class KafkaProducer {
  
  @Value("${spring.kafka.producer.topic-name}")
  private String topicName;
  
  private static final Logger LOGGER = LoggerFactory.getLogger(KafkaProducer.class);
  
  private final KafkaTemplate<String, Object> kafkaTemplate;
  
  @Autowired
  public KafkaProducer(KafkaTemplate<String, Object> kafkaTemplate) {
    this.kafkaTemplate = kafkaTemplate;
  }
  
  public String sendMessage(SpaceCollector message) {
    LOGGER.info("Message sent {}", message);
    kafkaTemplate.send(topicName, message);
    return "Message sent successfully";
  }
  
  public boolean writeMessage(FileMetadata message) {
    kafkaTemplate.send(topicName, message);
    LOGGER.info("Kafka status message {}", message);
    return true;
  }
  
  public void sendFailureMessage(SpaceCollector message, String fileType, ResponseEntity response) {
    FileMetadata metadata = new FileMetadata();
    metadata.setFile_name(message.getInputFilePath());
    metadata.setDate_received(new SimpleDateFormat("yyyy-MM-dd HH:mm:ss").format(new Date()));
    metadata.setDate_processed("");
    metadata.setNo_of_records_in_the_file(0);
    metadata.setNo_of_records_processed(0);
    metadata.setNo_of_records_failed(0);
    metadata.setJob_status("FAILURE");
    metadata.setJob_id("");
    metadata.setJar_id("");
    metadata.setExceptions(getErrorMessage(response));
    LOGGER.info("Failure message sent {}", metadata);
    this.writeMessage(metadata);
  }
  
  private String getErrorMessage(ResponseEntity response) {
    return "Job submission failed with status code " + response.getStatusCodeValue() +
           " and error message " + response.getBody().toString();
  }
}

FlinkApiController.java:

@Service
public class FlinkApiController {
  
  @Value("${flink.api.url}")
  private String flinkApiUrl;
  
  @Value("${flink.job.csv.jarid}")
  private String flinkJobCsvJarId;
  
  @Value("${flink.job.csv.program-args}")
  private String flinkJobCsvProgramArgs;
  
  @Value("${flink.job.xml.program-args}")
  private String flinkJobXmlProgramArgs;
  
  @Value("${flink.job.xml.jarid}")
  private String flinkJobXmlJarId;
  
  @Value("${flink.job.txt.jarid}")
  private String flinkJobTxtJarId;
  
  HttpHeaders headers = new HttpHeaders();
  
  private static final Logger LOGGER = LoggerFactory.getLogger(FlinkApiController.class);
  
  @Autowired
  private KafkaProducer kafkaProducer;
  
  @PostMapping
  public String triggerJob(SpaceCollector collector, String fileType) {
    RestTemplate restTemplate = new RestTemplate();
    LOGGER.info("TRIGGER JOB::::");
    HttpEntity<String> request = null;
    String jobSubmitUrl = null;
    
    if (fileType.equalsIgnoreCase(".csv")) {
      LOGGER.info("CSV FLINK TRIGGER");
      request = createRequest(flinkJobCsvJarId, flinkJobCsvProgramArgs.replace("input", collector.getInputFilePath()));
      jobSubmitUrl = flinkApiUrl + "/jars/" + flinkJobCsvJarId + "/run";

    } else if (fileType.equalsIgnoreCase(".xml")) {
      LOGGER.info("XML FLINK TRIGGER");
      String programXmlArgs = flinkJobXmlProgramArgs.replace("|filePath|", collector.getInputFilePath())
                                .replace("|primaryKey|", "");
      request = createRequest(flinkJobXmlJarId, programXmlArgs);
      jobSubmitUrl = flinkApiUrl + "/jars/" + flinkJobXmlJarId + "/run";
      LOGGER.info("jobSubmitUrl: {}", jobSubmitUrl);
      LOGGER.info("requestBody: {}", programXmlArgs);
      
    } else if (fileType.equalsIgnoreCase(".txt")) {
      LOGGER.info("TEXT FLINK TRIGGER");
      request = createRequest(flinkJobTxtJarId, flinkJobCsvProgramArgs.replace("|input|", collector.getInputFilePath()));
      jobSubmitUrl = flinkApiUrl + "/jars/" + flinkJobTxtJarId + "/run";
    }
    
    ResponseEntity<String> response = restTemplate.postForEntity(jobSubmitUrl, request, String.class);
    LOGGER.info("Response: {}", response);
    
    if (response.getStatusCode().is2xxSuccessful()) {
      return "Job triggered: " + response;
    } else {
      kafkaProducer.sendFailureMessage(collector, fileType, response);
      writeLogToFile(collector, fileType, response.getBody(), "shared/location");
      return "Job triggered failed";
    }
  }
  
  private HttpEntity<String> createRequest(String jarId, String programArgs) {
    headers.setContentType(MediaType.APPLICATION_JSON);
    JSONObject requestBody = new JSONObject();
    requestBody.put("programArgs", programArgs);
    return new HttpEntity<>(requestBody.toString(), headers);
  }
  
  private void writeLogToFile(SpaceCollector message, String fileType, String response, String destinationFolderPath) {
    String logFilePath = destinationFolderPath + "/log.txt";
    try (FileWriter fileWriter = new FileWriter(logFilePath, true)) {
       fileWriter.write(message.toString());
       fileWriter.write(System.lineSeparator());
       fileWriter.write(fileType);
       fileWriter.write(System.lineSeparator());
       fileWriter.write(response);
       fileWriter.write(System.lineSeparator());
       fileWriter.flush();
    } catch (IOException e) {
       LOGGER.error("Error writing log file", e);
    }
  }
}

KafkaConsumer.java:

@Service
public class KafkaConsumer {
  
  private static final Logger LOGGER = LoggerFactory.getLogger(KafkaConsumer.class);
  
  @Value("${file.txtsource}")
  private String sourceFilePath;
  
  @Value("${file.destination}")
  private String destinationFolderPath;
  
  @Autowired
  private FlinkApiController flinkApiController;
  
  @Autowired
  private KafkaProducer kafkaProducer;
  
  @KafkaListener(topics = "${spring.kafka.producer.topic-name}", groupId = "${spring.kafka.consumer.group-id}", errorHandler = "kafkaEventErrorHandler")
  public void consume(String data) {
    LOGGER.info("Data Received: {}", data);
    
    if (data.contains("inputFilePath")) {
      ObjectMapper mapper = new ObjectMapper();
      try {
        SpaceCollector message = mapper.readValue(data, SpaceCollector.class);
        LOGGER.info("Message Received: {}", message);
        String sourceFilePath = message.getInputFilePath();
        
        if (validateFile(sourceFilePath)) {
          String fileType = getExtension(sourceFilePath);
          writeFile(sourceFilePath, destinationFolderPath);

          if (fileType != null && fileType.equalsIgnoreCase(".xml")) {
            LOGGER.info("XML FILE");
            flinkApiController.triggerJob(message, fileType);

          } else if (fileType != null && fileType.equalsIgnoreCase(".csv")) {
            LOGGER.info("CSV FILE");
            flinkApiController.triggerJob(message, fileType);

          } else if (fileType != null && fileType.equalsIgnoreCase(".txt")) {
            LOGGER.info("TXT FILE");
            flinkApiController.triggerJob(message, fileType);
          }

          deleteFile(sourceFilePath);
        }
      } catch (IOException e) {
        LOGGER.error("Error reading the message", e);
      }
    }
  }
  
  private boolean validateFile(String sourceFilePath) {
    File file = new File(sourceFilePath);
    if (file.exists()) {
      return true;
    } else {
      LOGGER.error("Source file does not exist: {}", sourceFilePath);
      return false;
    }
  }
  
  private void writeFile(String sourceFilePath, String destinationFolderPath) {
    try {
      Files.copy(Paths.get(sourceFilePath), Paths.get(destinationFolderPath), StandardCopyOption.REPLACE_EXISTING);
    } catch (IOException e) {
      LOGGER.error("Error writing the file", e);
    }
  }
  
  private void deleteFile(String sourceFilePath) {
    try {
      Files.deleteIfExists(Paths.get(sourceFilePath));
    } catch (IOException e) {
      LOGGER.error("Error deleting the file", e);
    }
  }
  
  private String getExtension(String sourceFilePath) {
    if (sourceFilePath != null && sourceFilePath.contains(".")) {
      return sourceFilePath.substring(sourceFilePath.lastIndexOf("."));
    }
    return null;
  }
}