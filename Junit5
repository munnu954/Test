Here are the updated classes:

JobStatus class:
```java
public enum JobStatus {
    COLLECTION_NEW,
    COLLECTION_IN_PROGRESS,
    COLLECTION_SUCCESSFUL,
    COLLECTION_FAILED,
    FLINK_JOB_NEW,
    FLINK_JOB_SUBMITTED,
    FLINK_JOB_IN_PROGRESS,
    FLINK_JOB_SUCCESSFUL,
    FLINK_JOB_FAILED
}
```

KafkaAuditProducer class:
```java
@Service
public class KafkaAuditProducer {
    private static final Logger Logger = LoggerFactory.getLogger(KafkaAuditProducer.class);

    @Value("${spring.kafka.producer.topic-name}")
    private String topicName;

    private final KafkaTemplate<String, Object> kafkaTemplate;

    @Autowired
    public KafkaAuditProducer(KafkaTemplate<String, Object> kafkaTemplate) {
        this.kafkaTemplate = kafkaTemplate;
    }

    public String sendMessage(UnifiedAuditMessage auditMsg) {
        Logger.info(String.format("Message sent %s", auditMsg));
        kafkaTemplate.send(topicName, auditMsg);
        return "Message sent successfully";
    }

    public void createMessageAndLog(String inputFilePath, String url, Integer port, String outputFilePath, String delimiters, String fileType, String date_received,
                                    Integer no_of_records_in_the_file, Integer no_of_records_processed, Integer no_of_records_failed, List<String> exceptions,
                                    String job_id, String jar_id, String collector_id, JobStatus jobStatus) {
        LocalDateTime dateProcessed = LocalDateTime.now();
        String date_processed = dateProcessed.format(DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss.SSS"));
        UnifiedAuditMessage auditMsg = new UnifiedAuditMessage(inputFilePath, url, port, outputFilePath, delimiters, fileType, date_received,
                date_processed, no_of_records_in_the_file, no_of_records_processed, no_of_records_failed, job_id, jar_id, collector_id, exceptions);
        auditMsg.setJob_status(jobStatus.toString());

        try {
            ObjectMapper objectMapper = new ObjectMapper();
            String auditMsgJson = objectMapper.writeValueAsString(auditMsg); 
            this.kafkaTemplate.send(topicName, auditMsgJson);
            Logger.info("Kafka status message {}", auditMsgJson);
        } catch (JsonProcessingException e) {
            Logger.error("Failed to convert auditMsg to JSON: {}", e.getMessage());
        } catch (Exception e) { 
            Logger.error("Failed to send Kafka message: {}", e.getMessage());
        }
    }
}
```

FlinkApiController class:
```java
@Service
public class FlinkApiController {

    @Value("${flink.api.url}")
    private String flinkApiUrl;

    @Value("${flink.job.csv.jarid}")
    private String flinkJobJarid;

    @Value("${flink.job.csv.program-args)")
    private String programArgs;

    @Value("${flink.job.txt.jarid}")
    private String flinkJobtxtJarid;

    @Value("${flink.job.xml.program-args)")
    private String programXmlArgs;

    @Value("${flink.job.xml.jarid}")
    private String flinkJobXmlJarid;

    @Value("${log.directory.path)")
    private String logFolderPath;

    private KafkaConsumer KafkaConsumer = new KafkaConsumer ();
    @Autowired
    private KafkaAuditProducer kafkaAuditProducer;
    HttpHeaders headers = new HttpHeaders();

    private static final Logger LOGGER = LoggerFactory.getLogger(FlinkApiController.class);

    @PostMapping
    public String trigger Job(UnifiedAuditMessage auditMsg, String fileType) {
        RestTemplate restTemplate = new RestTemplate();
        LOGGER.info("TRIGGER JOB::::");
        HttpEntity<String> request = null;
        String jobSubmitUrl = null;
        if (programArgs != null && !programArgs.isEmpty()) {
            headers.setContentType(MediaType.APPLICATION_JSON);
            JSONObject requestBody = new JSONObject();

            try {
                if (fileType.equalsIgnoreCase(".csv")) {
                    requestBody.put("programArgs", programArgs.replace("input", auditMsg.getInputFilePath()));
                    jobSubmitUrl = flinkApiUrl + "/jars/" + flinkJobJarid + "/run";

                } else if (fileType.equalsIgnoreCase(".xml")) {
                    LOGGER.info("XML FLINK TRIGGER::");
                    programXmlArgs = programXmlArgs.replace("|filePath|", auditMsg.getInputFilePath());
                    programXmlArgs = programXmlArgs.replace(" |primaryKey|", "");
                    requestBody.put("programArgs", programXmlArgs);
                    jobSubmitUrl = flinkApiUrl + "/jars/" + flinkJobXmlJarid + "/run";
                    LOGGER.info("jobSubmitUrl:" + jobSubmitUrl);
                    LOGGER.info("requestBody::" + requestBody.toString());
                } else if (fileType.equalsIgnoreCase(".txt")) {
                    LOGGER.info("TEXT FLINK TRIGGER::");
                    requestBody.put("programArgs", programArgs.replace("|input|", auditMsg.getInputFilePath()));
                    jobSubmitUrl = flinkApiUrl + "/jars/" + flinkJobtxtJarid + "/run";
                }

                request = new HttpEntity<String>(requestBody.toString(), headers);
            } catch (Exception e) {
                e.printStackTrace();
            }
        }

        // Trigger the submitted jar
        ResponseEntity<String> response = restTemplate.postForEntity(jobSubmitUrl, request, String.class);
        LOGGER.info("response: {}" + response);

        if (response != null && response.getStatusCode() == HttpStatus.OK) {
            String jobID = response.getBody();
            LOGGER.info("Flink Job ID: {}", jobID);

            try {
                JsonNode jsonNode = restTemplate.getForObject(flinkApiUrl + jobID, JsonNode.class);
                while (true) {
                    if (jsonNode.get("state").textValue().equals("FINISHED")
                            || jsonNode.get("state").textValue().equals("CANCELED")
                            || jsonNode.get("state").textValue().equals("FAILED")
                            || jsonNode.get("state").textValue().equals("SUSPENDED")) {
                        break;
                    }
                    Thread.sleep(100);
                }
                JobStatus jobStatus = JobStatus.FLINK_JOB_FAILED;
                if (jsonNode.get("state").textValue().equals("FINISHED")) {
                    jobStatus = JobStatus.FLINK_JOB_SUBMITTED;
                }
                kafkaAuditProducer.createMessageAndLog(auditMsg.getInputFilePath(), auditMsg.getUrl(), auditMsg.getPort(),
                        auditMsg.getOutputFilePath(), auditMsg.getDelimiters(), fileType, new SimpleDateFormat("yyyy-MM-dd HH:mm:ss").format(new Date()),
                        auditMsg.getNo_of_records_in_the_file(), auditMsg.getNo_of_records_processed(), auditMsg.getNo_of_records_failed(),
                        auditMsg.getExceptions(), auditMsg.getJob_id(), auditMsg.getJar_id(), auditMsg.getCollector_id(), jobStatus