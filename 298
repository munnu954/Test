Sure, I can create a new entity class called "FileMetadata" to hold the additional fields and update the existing classes accordingly\.

FileMetadata\.java:

```java
public class FileMetadata {
    private String fileName;
    private String dateReceived;
    private String dateProcessed;
    private int noOfRecordsInFile;
    private int noOfRecordsProcessed;
    private int noOfRecordsFailed;
    private String jobStatus;
    private String jobId;
    private String jarId;
    private String exceptions;

    // constructors, getters and setters
}
```

SpaceCollector\.java:

```java
public class SpaceCollector {
    private String fileType;
    private String fileContent;
    private String sourceSystem;
    private FileMetadata fileMetadata;

    // constructors, getters and setters
}
```

JsonUtils\.java, KafkaProducer\.java, and KafkaController\.java remain unchanged\.

JsonUtils\.java:

```java
public class JsonUtils {
    private static final ObjectMapper objectMapper = new ObjectMapper();

    public static String toJson(Object object) throws JsonProcessingException {
        return objectMapper.writeValueAsString(object);
    }

    public static <T> T fromJson(String json, Class<T> clazz) throws JsonProcessingException {
        return objectMapper.readValue(json, clazz);
    }
}
```

KafkaProducer\.java:

```java
public class KafkaProducer {
    @Autowired
    private KafkaTemplate<String, String> kafkaTemplate;

    public void sendMessage(String topic, String message) {
        kafkaTemplate.send(topic, message);
    }
}
```

KafkaController\.java:

```java
@RestController
@RequestMapping("/kafka")
public class KafkaController {
    @Autowired
    private KafkaProducer kafkaProducer;

    @PostMapping("/send")
    public void sendMessageToKafkaTopic(@RequestParam("topic") String topic, @RequestBody SpaceCollector spaceCollector) throws JsonProcessingException {
        String message = JsonUtils.toJson(spaceCollector);
        kafkaProducer.sendMessage(topic, message);
    }
}

FlinkApiController\.java:

```java
@RestController
@RequestMapping("/flink")
public class FlinkApiController {
    private static final Logger LOGGER = LoggerFactory.getLogger(FlinkApiController.class);

    @Autowired
    private RestTemplate restTemplate;

    @Value("${destination.folder}")
    private String destinationFolder;

    @Autowired
    private KafkaProducer kafkaProducer;

    @PostMapping("/transform")
    public ResponseEntity<String> transformFile(@RequestBody SpaceCollector spaceCollector) throws Exception {
        FileMetadata fileMetadata = spaceCollector.getFileMetadata();
        String fileType = spaceCollector.getFileType();
        String url = "http://flink-cluster:8081/jars/upload";
        HttpHeaders headers = new HttpHeaders();
        headers.setContentType(MediaType.MULTIPART_FORM_DATA);

        // check if jar file exists
        if (!checkJarExists(fileMetadata.getJarId())) {
            LOGGER.error("Jar file not found for job with id: {}", fileMetadata.getJobId());
            pushFailureMessageToAuditQueue(fileMetadata, "Jar file not found");
            writeToFile("Jar file not found for job with id: " + fileMetadata.getJobId());
            return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR).body("Jar file not found");
        }

        // upload jar file to Flink cluster
        MultiValueMap<String, Object> body = new LinkedMultiValueMap<>();
        body.add("jarfile", new FileSystemResource("/jars/" + fileMetadata.getJarId()));
        HttpEntity<MultiValueMap<String, Object>> requestEntity = new HttpEntity<>(body, headers);
        ResponseEntity<String> responseEntity = restTemplate.exchange(url, HttpMethod.POST, requestEntity, String.class);
        if (responseEntity.getStatusCode() != HttpStatus.CREATED) {
            LOGGER.error("Error uploading jar file for job with id: {}", fileMetadata.getJobId());
            pushFailureMessageToAuditQueue(fileMetadata, "Error uploading jar file");
            writeToFile("Error uploading jar file for job with id: " + fileMetadata.getJobId());
            return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR).body("Error uploading jar file");
        }

        // submit Flink job based on file type
        switch (fileType) {
            case "csv":
                // submit CSV job
                break;
            case "xml":
                // submit XML job
                break;
            case "txt":
                // submit TXT job
                break;
            default:
                LOGGER.error("Invalid file type: {}", fileType);
                pushFailureMessageToAuditQueue(fileMetadata, "Invalid file type");
                writeToFile("Invalid file type for job with id: " + fileMetadata.getJobId());
                return ResponseEntity.status(HttpStatus.BAD_REQUEST).body("Invalid file type");
        }

        return ResponseEntity.ok().build();
    }

    private boolean checkJarExists(String jarId) {
        // check if jar file exists on Flink cluster
        // return true if found, false otherwise
    }

    private void pushFailureMessageToAuditQueue(FileMetadata fileM

etadata, String errorMessage) throws JsonProcessingException {
        fileMetadata.setNoOfRecordsProcessed(0);
        fileMetadata.setJobStatus("Failed");
        fileMetadata.setExceptions(errorMessage);
        SpaceCollector spaceCollector = new SpaceCollector();
        spaceCollector.setFileMetadata(fileMetadata);
        String message = JsonUtils.toJson(spaceCollector);
        try {
            kafkaProducer.sendMessage("3459-COLLECTION-AUDIT-QUEUE", message);
        } catch (Exception e) {
            LOGGER.error("Error publishing failure message to Kafka topic: {}", e.getMessage());
            writeToFile("Error publishing failure message to Kafka topic: " + e.getMessage());
        }
    }

    private void writeToFile(String message) {
        try {
            Files.write(Paths.get(destinationFolder + "/log.txt"), message.getBytes(), StandardOpenOption.APPEND);
        } catch (IOException e) {
            LOGGER.error("Error writing to log file: {}", e.getMessage());
        }
    }
}
```

KafkaConsumer\.java:

```java
@Service
public class KafkaConsumer {
    private static final Logger LOGGER = LoggerFactory.getLogger(KafkaConsumer.class);

    @Value("${destination.folder}")
    private String destinationFolder;

    @KafkaListener(topics = "3459-COLLECTION-REQUEST-QUEUE", groupId = "collection-group")
    public void consume(String message) {
        LOGGER.info("Received message: {}", message);
        writeToFile(message);
        // process message
    }

    private void writeToFile(String message) {
        try {
            Files.write(Paths.get(destinationFolder + "/log.txt"), message.getBytes(), StandardOpenOption.APPEND);
        } catch (IOException e) {
            LOGGER.error("Error writing to log file: {}", e.getMessage());
        }
    }
}
```