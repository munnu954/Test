To fix the "com.fasterxml.jackson.databind.exc.MismatchedInputException: Cannot construct instance of org.vdsi.space.collections.customcsvdatatransformer.model.Unified AuditMessage" error, you need to add a default (no-args) constructor to your UnifiedAuditMessage class. This is because the ObjectMapper requires a no-args constructor to create an instance of the class during deserialization.

Here's the updated code for the UnifiedAuditMessage class:

```java
@Data
@AllArgsConstructor
@NoArgsConstructor
public class UnifiedAuditMessage {
    // existing variables and methods
    // add a default constructor
}
```

Additionally, there are a few corrections and improvements needed in the KafkaAuditProducer and KafkaConsumer classes to make them work properly.

Updated KafkaAuditProducer class:

```java
@Service
public class KafkaAuditProducer {
    private static final Logger Logger = LoggerFactory.getLogger(KafkaAuditProducer.class);

    private final KafkaTemplate<String, String> kafkaTemplate;

    @Value("${spring.kafka.producer.topic-name}")
    private String topicName;

    @Autowired
    public KafkaAuditProducer(KafkaTemplate<String, String> kafkaTemplate) {
        this.kafkaTemplate = kafkaTemplate;
    }

    public void createMessageAndLog(String inputFilePath, String url, Integer port, String outputFilePath,
            String delimiters, String fileType, String date_received, Integer no_of_records_in_the_file,
            Integer no_of_records_processed, Integer no_of_records_failed, UUID job_id, UUID jar_id,
            List<String> exceptions, UUID collector_id, UnifiedAuditMessage.JobStatus job_status) {
        LocalDateTime dateProcessed = LocalDateTime.now();
        String date_processed = dateProcessed.format(DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss.SSS"));

        UnifiedAuditMessage auditMsg = new UnifiedAuditMessage(inputFilePath, url, port, outputFilePath, delimiters,
                fileType, date_received, date_processed, no_of_records_in_the_file, no_of_records_processed,
                no_of_records_failed, job_id, jar_id, exceptions, collector_id, job_status);

        try {
            ObjectMapper objectMapper = new ObjectMapper();
            String auditMsgJson = objectMapper.writeValueAsString(auditMsg);

            this.kafkaTemplate.send(topicName, auditMsgJson);
            Logger.info("Kafka status message: {}", auditMsgJson);
        } catch (JsonProcessingException e) {
            Logger.error("Failed to convert auditMsg to JSON: {}", e.getMessage());
        } catch (Exception e) {
            Logger.error("Failed to send Kafka message: {}", e.getMessage());
        }
    }

    public boolean writeMessage(UnifiedAuditMessage auditMsg) {
        try {
            ObjectMapper objectMapper = new ObjectMapper();
            String auditMsgJson = objectMapper.writeValueAsString(auditMsg);

            this.kafkaTemplate.send(topicName, auditMsgJson);
            Logger.info("Kafka status message: {}", auditMsgJson);
            return true;
        } catch (JsonProcessingException e) {
            Logger.error("Failed to convert auditMsg to JSON: {}", e.getMessage());
        } catch (Exception e) {
            Logger.error("Failed to send Kafka message: {}", e.getMessage());
        }
        return false;
    }
}
```

Updated KafkaConsumer class:

```java
@Service
public class KafkaConsumer {
    private static final Logger LOGGER = LoggerFactory.getLogger(KafkaConsumer.class);

    @Value("${file.txtsource}")
    private String sourceFilePath;

    @Value("${file.destination}")
    private String destinationFolderPath;

    @Value("${spring.kafka.producer.topic-name}")
    private String topic;

    @Autowired
    private KafkaTopicUtil util;

    @Autowired
    private FlinkApiController FlinkApi;

    @Autowired
    private KafkaAuditProducer kafkaAuditProducer;

    @KafkaListener(topics = "${spring.kafka.producer.topic-name}", groupId = "${spring.kafka.consumer.group-id}", errorHandler = "kafkaEventErrorHandler")
    public void consume(String data) throws IOException {
        LocalDateTime dateReceived = LocalDateTime.now();
        UnifiedAuditMessage auditMsg = null;
        String fileType = null;

        try {
            ObjectMapper mapper = new ObjectMapper();
            auditMsg = mapper.readValue(data, UnifiedAuditMessage.class);
            LOGGER.info("Message Received: {}", auditMsg);

            if (data.contains("inputFilePath")) {
                LOGGER.info("Message received-> {}", auditMsg);

                String sourceFilePath = auditMsg.getInputFilePath();
                String response = null;

                if (!new File(sourceFilePath).exists()) {
                    UnifiedAuditMessage auditData = new UnifiedAuditMessage();
                    auditData.setInputFilePath(sourceFilePath);
                    auditData.setExceptions(Collections.singletonList("Input file path doesn't exist/Invalid file"));
                    writeLogToFile(auditData, destinationFolderPath);
                } else {
                    if (validateFile(sourceFilePath)) {
                        writeFile(sourceFilePath, destinationFolderPath);
                        String fileType = getExtension(sourceFilePath);

                        if (fileType != null && fileType.equalsIgnoreCase(".xml")) {
                            LOGGER.info("XML FILE()");
                            retrieveXmlFile(sourceFilePath);
                        } else if (fileType != null && fileType.equalsIgnoreCase(".csv")) {
                            LOGGER.info("CSV FILE()");
                            retrieveCsvFile(sourceFilePath);
                        } else if (fileType != null && fileType.equalsIgnoreCase(".txt")) {
                            LOGGER.info("TEXT FILE()");
                            retrieveTxtFile(sourceFilePath);
                        }
                        response = getFlinkResponse();
                        LOGGER.info("Flink Response: {}", response);
                    } else {
                        kafkaAuditProducer.createMessageAndLog(sourceFilePath, null, null, null, null, fileType, null,
                                null, null, null, null, null, Collections.singletonList("File does not exist"),
                                null, UnifiedAuditMessage.JobStatus.COLLECTION_FAILED);
                    }
                }
            }
        } catch (Exception e) {
            e.printStackTrace();
            LOGGER.error("Error triggering job: {}", e.getMessage());

            UnifiedAuditMessage auditData = logAuditData(auditMsg, fileType, e);
            kafkaAuditProducer.createMessageAndLog(auditData.getInputFilePath(), auditData.getUrl(),
                    auditData.getPort(), auditData.getOutputFilePath(), auditData.getDelimiters(),
                    auditData.getFileType(), auditData.getDate_received(), auditData.getNo_of_records_in_the_file(),
                    auditData.getNo_of_records_processed(), auditData.getNo_of_records_failed(), auditData.getJob_id(),
                    auditData.getJar_id(), auditData.getExceptions(), auditData.getCollector_id(),
                    auditData.getJob_status());
        }
    }

    public UnifiedAuditMessage logAuditData(UnifiedAuditMessage auditMsg, String fileType, Exception e) {
        DateTimeFormatter formatter = DateTimeFormatter.ofPattern("yyyy-MM-dd'T'HH:mm:ss");
        LocalDateTime currentDate = LocalDateTime.now();

        return new UnifiedAuditMessage(auditMsg.getInputFilePath(), auditMsg.getUrl(), auditMsg.getPort(),
                auditMsg.getOutputFilePath(), auditMsg.getDelimiters(), fileType, currentDate.format(formatter),
                currentDate.format(formatter), auditMsg.getNo_of_records_in_the_file(),
                auditMsg.getNo_of_records_processed(), auditMsg.getNo_of_records_failed(), UUID.randomUUID(),
                UUID.randomUUID(), Collections.singletonList(e.getMessage()), auditMsg.getCollector_id(),
                UnifiedAuditMessage.JobStatus.COLLECTION_FAILED);
    }

    // Other methods go here...
}
```

These updated classes should resolve the error and make the code work as expected. Remember to update the other methods and variables in the KafkaConsumer class accordingly.