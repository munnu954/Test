To implement JobStatus class and add AC1 and AC2 to the code, you can modify the existing code as follows:

1. Create a new class named JobStatus with the following enum values:

```java
public enum JobStatus {
    COLLECTION_NEW,
    COLLECTION_IN_PROGRESS,
    COLLECTION_SUCCESSFUL,
    COLLECTION_FAILED,
    FLINK_JOB_NEW,
    FLINK_JOB_SUBMITTED,
    FLINK_JOB_IN_PROGRESS,
    FLINK_JOB_SUCCESSFUL,
    FLINK_JOB_FAILED
}
```

2. Create a new class named Utils (or any other name you prefer) to handle setting the JobStatus. Add the following method to set the JobStatus to FLINK_JOB_SUBMITTED:

```java
public class Utils {
    public static UnifiedAuditMessage setJobStatusSubmitted(UnifiedAuditMessage auditMsg) {
        auditMsg.setJob_status(JobStatus.FLINK_JOB_SUBMITTED.name());
        return auditMsg;
    }
    
    public static UnifiedAuditMessage setJobStatusFailed(UnifiedAuditMessage auditMsg) {
        auditMsg.setJob_status(JobStatus.FLINK_JOB_FAILED.name());
        return auditMsg;
    }
}
```

3. Modify the FlinkApiController class to use the Utils class and set the JobStatus accordingly. Add the following import statement:

```java
import static your.package.name.Utils.*;
```

Inside the `triggerJob` method, update the code as follows:

```java
// Trigger the submitted jar
ResponseEntity<String> response = restTemplate.postForEntity(jobSubmitUrl, request, String.class);
LOGGER.info("response: {}"+response);

if (response != null && response.getStatusCode() != HttpStatus.OK) {
    return "Job submitted";
} else {
    LOGGER.info("Not Ok!!");

    UnifiedAuditMessage auditData = kafkaConsumer.logMetadata(auditMsg, fileType, response);
    auditData = setJobStatusSubmitted(auditData);

    kafkaAuditProducer.createMessageAndLog(auditData.getInputFilePath(), auditData.getUrl(), auditData.getPort(),
            auditData.getOutputFilePath(), auditData.getDelimiters(), fileType,
            new SimpleDateFormat("yyyy-MM-dd HH:mm:ss").format(new Date()),
            auditData.getNo_of_records_in_the_file(), auditData.getNo_of_records_processed(),
            auditData.getNo_of_records_failed(), auditData.getJob_id(), auditData.getJar_id(),
            auditData.getCollector_id(), auditData.getExceptions());

    kafkaConsumer.writeLogToFile(auditData, fileType, response, logFolderPath);
}

```

4. Update the KafkaConsumer class to use the Utils class and set the JobStatus accordingly. Add the following import statement:

```java
import static your.package.name.Utils.*;
```

Inside the `consume` method, update the code as follows:

```java
// Trigger the submitted jar
ResponseEntity<String> response = restTemplate.postForEntity(jobSubmitUrl, request, String.class);
LOGGER.info("response: {}"+response);

if (response != null && response.getStatusCode() != HttpStatus.OK) {
    return "Job submitted";
} else {
    LOGGER.info("Not Ok!!");

    UnifiedAuditMessage auditData = logMetadata(auditMsg, fileType, response);
    auditData = setJobStatusFailed(auditData);

    kafkaAuditProducer.createMessageAndLog(auditData.getInputFilePath(), auditData.getUrl(), auditData.getPort(),
            auditData.getOutputFilePath(), auditData.getDelimiters(), fileType,
            new SimpleDateFormat("yyyy-MM-dd HH:mm:ss").format(new Date()),
            auditData.getNo_of_records_in_the_file(), auditData.getNo_of_records_processed(),
            auditData.getNo_of_records_failed(), auditData.getJob_id(), auditData.getJar_id(),
            auditData.getCollector_id(), auditData.getExceptions());

    utils.buildAuditQueueJSON(JobStatus.FLINK_JOB_FAILED);
}
```

Note that you need to replace "your.package.name" with the actual package name where the Utils class is located.