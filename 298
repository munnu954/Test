import org.junit.jupiter.api.BeforeEach;
import org.junit.jupiter.api.Test;
import org.mockito.InjectMocks;
import org.mockito.Mock;
import org.mockito.MockitoAnnotations;
import org.springframework.boot.test.context.SpringBootTest;
import org.springframework.kafka.core.KafkaTemplate;
import org.springframework.util.LinkedMultiValueMap;
import org.springframework.util.MultiValueMap;
import org.springframework.web.client.HttpClientErrorException;
import org.springframework.web.client.RestTemplate;

import java.io.File;
import java.io.FileWriter;
import java.io.IOException;
import java.text.SimpleDateFormat;
import java.util.Date;

import static org.junit.jupiter.api.Assertions.*;
import static org.mockito.ArgumentMatchers.*;
import static org.mockito.Mockito.*;

@SpringBootTest
public class KafkaInputProducerTest {

    @InjectMocks
    private KafkaInputProducer kafkaInputProducer;

    @Mock
    private KafkaTemplate<String, Object> kafkaTemplate;

    @BeforeEach
    public void setup() {
        MockitoAnnotations.openMocks(this);
    }

    @Test
    public void testWriteMessage_Successful() {
        String topicName = "test-topic";
        FileMetadata collector = new FileMetadata();
        collector.setFileName("test.csv");

        when(kafkaTemplate.send(eq(topicName), anyObject())).thenReturn(null);

        boolean result = kafkaInputProducer.writeMessage(collector);

        assertTrue(result);
        verify(kafkaTemplate, times(1)).send(eq(topicName), eq(collector));
    }

    @Test
    public void testWriteMessage_Failure() {
        String topicName = "test-topic";
        FileMetadata collector = new FileMetadata();
        collector.setFileName("test.csv");

        when(kafkaTemplate.send(eq(topicName), anyObject())).thenThrow(new RuntimeException());

        boolean result = kafkaInputProducer.writeMessage(collector);

        assertFalse(result);
        verify(kafkaTemplate, times(1)).send(eq(topicName), eq(collector));
    }
}

@SpringBootTest
public class FlinkApiControllerTest {

    @InjectMocks
    private FlinkApiController flinkApiController;

    @Mock
    private RestTemplate restTemplate;

    @Mock
    private KafkaConsumer kafkaConsumer;

    @Mock
    private KafkaInputProducer kafkaInputProducer;

    @BeforeEach
    public void setup() {
        MockitoAnnotations.openMocks(this);
    }

    @Test
    public void testTriggerJob_Successful() {
        String flinkApiUrl = "http://localhost:8081";
        String flinkJobJarid = "job1";
        String programArgs = "--input path/to/file.csv";
        String fileType = ".csv";
        SpaceCollector collector = new SpaceCollector();
        collector.setInputFilePath("path/to/file.csv");

        HttpHeaders headers = new HttpHeaders();
        headers.setContentType(MediaType.APPLICATION_JSON);

        MultiValueMap<String, String> requestBody = new LinkedMultiValueMap<>();
        requestBody.add("programArgs", programArgs.replace("input", collector.getInputFilePath()));

        HttpEntity<MultiValueMap<String, String>> request = new HttpEntity<>(requestBody, headers);

        String jobSubmitUrl = flinkApiUrl + "/jars/" + flinkJobJarid + "/run";

        when(restTemplate.postForEntity(eq(jobSubmitUrl), eq(request), eq(String.class)))
                .thenReturn(new ResponseEntity<>("SUCCESS", HttpStatus.OK));

        String result = flinkApiController.triggerJob(collector, fileType);

        assertEquals("SUCCESS", result);
        verify(restTemplate, times(1)).postForEntity(eq(jobSubmitUrl), eq(request), eq(String.class));
        verify(kafkaConsumer, never()).logMetadata(any(SpaceCollector.class), anyString(), any(Exception.class));
        verify(kafkaInputProducer, never()).sendFailureMessage(any(SpaceCollector.class), anyString(), any(ResponseEntity.class));
        verify(kafkaConsumer, never()).writeLogToFile(any(FileMetadata.class), anyString(), any(ResponseEntity.class), anyString());
    }

    @Test
    public void testTriggerJob_JobSubmissionError() {
        String flinkApiUrl = "http://localhost:8081";
        String flinkJobJarid = "job1";
        String programArgs = "--input path/to/file.csv";
        String fileType = ".csv";
        SpaceCollector collector = new SpaceCollector();
        collector.setInputFilePath("path/to/file.csv");

        HttpHeaders headers = new HttpHeaders();
        headers.setContentType(MediaType.APPLICATION_JSON);

        MultiValueMap<String, String> requestBody = new LinkedMultiValueMap<>();
        requestBody.add("programArgs", programArgs.replace("input", collector.getInputFilePath()));

        HttpEntity<MultiValueMap<String, String>> request = new HttpEntity<>(requestBody, headers);

        String jobSubmitUrl = flinkApiUrl + "/jars/" + flinkJobJarid + "/run";

        when(restTemplate.postForEntity(eq(jobSubmitUrl), eq(request), eq(String.class)))
                .thenThrow(new HttpClientErrorException(HttpStatus.BAD_REQUEST, "Job submission failed"));

        String result = flinkApiController.triggerJob(collector, fileType);

        assertEquals("Job submission failed", result);
        verify(restTemplate, times(1)).postForEntity(eq(jobSubmitUrl), eq(request), eq(String.class));
        verify(kafkaConsumer, times(1)).logMetadata(eq(collector), eq(fileType), any(Exception.class));
        verify(kafkaInputProducer, times(1)).sendFailureMessage(eq(collector), eq(fileType), any(Exception.class));
        verify(kafkaConsumer, times(1)).writeLogToFile(any(FileMetadata.class), anyString(), any(ResponseEntity.class), anyString());
    }
}

@SpringBootTest
public class KafkaConsumerTest {

    @InjectMocks
    private KafkaConsumer kafkaConsumer;

    @Mock
    private KafkaTopicUtil kafkaTopicUtil;

    @Mock
    private FlinkApiController flinkApiController;

    @Mock
    private KafkaAuditProducer kafkaAuditProducer;

    @Mock
    private KafkaInputProducer kafkaInputProducer;

    @BeforeEach
    public void setup() {
        MockitoAnnotations.openMocks(this);
        kafkaConsumer.setDestinationFolderPath("path/to/log/folder");
    }

    @Test
    public void testConsume_Successful() throws IOException {
        String topicName = "test-topic";
        String sourceFilePath = "path/to/file.csv";
        FileMetadata metadata = new FileMetadata();
        metadata.setFileName("test.csv");

        SpaceCollector collector = new SpaceCollector();
        collector.setInputFilePath(sourceFilePath);

        String data = "{ \"inputFilePath\": \"" + sourceFilePath + "\" }";

        when(kafkaTopicUtil.isKafkaTopicPresent(eq(topicName))).thenReturn(true);
        when(flinkApiController.triggerJob(eq(collector), eq(".csv"))).thenReturn("SUCCESS");

        kafkaConsumer.consume(data);

        verify(flinkApiController, times(1)).triggerJob(eq(collector), eq(".csv"));
        verify(kafkaInputProducer, never()).sendFailureMessage(any(SpaceCollector.class), anyString(), any(Exception.class));
        verify(kafkaConsumer, never()).writeLogToFile(any(FileMetadata.class), anyString());
    }

    @Test
    public void testConsume_JobSubmissionFailure() throws IOException {
        String topicName = "test-topic";
        String sourceFilePath = "path/to/file.csv";
        FileMetadata metadata = new FileMetadata();
        metadata.setFileName("test.csv");

        SpaceCollector collector = new SpaceCollector();
        collector.setInputFilePath(sourceFilePath);

        String data = "{ \"inputFilePath\": \"" + sourceFilePath + "\" }";

        when(kafkaTopicUtil.isKafkaTopicPresent(eq(topicName))).thenReturn(true);
        when(flinkApiController.triggerJob(eq(collector), eq(".csv"))).thenReturn("Job submission failed");
        doNothing().when(kafkaConsumer).writeLogToFile(any(FileMetadata.class), anyString());

        kafkaConsumer.consume(data);

        verify(flinkApiController, times(1)).triggerJob(eq(collector), eq(".csv"));
        verify(kafkaInputProducer, times(1)).sendFailureMessage(eq(collector), eq(".csv"), any(Exception.class));
        verify(kafkaConsumer, times(1)).writeLogToFile(any(FileMetadata.class), anyString());
    }

    @Test
    public void testConsume_TopicNotAccessible() throws IOException {
        String topicName = "test-topic";
        String sourceFilePath = "path/to/file.csv";
        FileMetadata metadata = new FileMetadata();
        metadata.setFileName("test.csv");

        SpaceCollector collector = new SpaceCollector();
        collector.setInputFilePath(sourceFilePath);

        String data = "{ \"inputFilePath\": \"" + sourceFilePath + "\" }";

        when(kafkaTopicUtil.isKafkaTopicPresent(eq(topicName))).thenReturn(false);
        doNothing().when(kafkaConsumer).writeLogToFile(any(FileMetadata.class), anyString());
        doNothing().when(kafkaAuditProducer).sendMessage(any(FileMetadata.class));

        kafkaConsumer.consume(data);

        verify(kafkaInputProducer, never()).sendFailureMessage(any(SpaceCollector.class), anyString(), any(Exception.class));
        verify(kafkaConsumer, times(1)).writeLogToFile(any(FileMetadata.class), anyString());
        verify(kafkaAuditProducer, times(1)).sendMessage(any(FileMetadata.class));
    }

    @Test
    public void testConsume_Failure() throws IOException {
        String topicName = "test-topic";
        String sourceFilePath = "path/to/file.csv";
        FileMetadata metadata = new FileMetadata();
        metadata.setFileName("test.csv");

        SpaceCollector collector = new SpaceCollector();
        collector.setInputFilePath(sourceFilePath);

        String data = "{ \"inputFilePath\": \"" + sourceFilePath + "\" }";

        when(kafkaTopicUtil.isKafkaTopicPresent(eq(topicName))).thenReturn(true);
        when(flinkApiController.triggerJob(eq(collector), eq(".csv"))).thenThrow(new RuntimeException());
        doNothing().when(kafkaConsumer).writeLogToFile(any(FileMetadata.class), anyString());

        kafkaConsumer.consume(data);

        verify(flinkApiController, times(1)).triggerJob(eq(collector), eq(".


----------_----------6666------------66------
import org.junit.jupiter.api.Test;
import org.junit.jupiter.api.extension.ExtendWith;
import org.mockito.ArgumentCaptor;
import org.mockito.InjectMocks;
import org.mockito.Mock;
import org.mockito.junit.jupiter.MockitoExtension;
import org.springframework.kafka.core.KafkaTemplate;
import org.springframework.util.Assert;

import java.io.File;
import java.io.IOException;
import java.nio.file.Files;
import java.nio.file.Paths;

import static org.mockito.ArgumentMatchers.any;
import static org.mockito.Mockito.verify;

@ExtendWith(MockitoExtension.class)
public class KafkaInputProducerTest {

    @Mock
    private KafkaTemplate<String, Object> kafkaTemplate;

    @InjectMocks
    private KafkaInputProducer kafkaInputProducer;

    @Test
    public void testWriteMessageSuccess() {
        // Arrange
        FileMetadata collector = new FileMetadata();

        // Act
        boolean result = kafkaInputProducer.writeMessage(collector);

        // Assert
        Assert.isTrue(result, "Message should be successfully written");
        verify(kafkaTemplate).send(any(), any());
    }

    @Test
    public void testWriteMessageFailure() {
        // Arrange
        FileMetadata collector = new FileMetadata();
        String expectedError = "Failed to send message";

        // Set up the mock to throw an exception when send method is called
        Mockito.doThrow(new RuntimeException(expectedError))
                .when(kafkaTemplate).send(any(), any());

        // Act
        boolean result = kafkaInputProducer.writeMessage(collector);

        // Assert
        Assert.isFalse(result, "Message should fail to be written");
        verify(kafkaTemplate).send(any(), any());

        // Verify the captured log message
        ArgumentCaptor<String> logCaptor = ArgumentCaptor.forClass(String.class);
        verify(Logger).info(logCaptor.capture());
        Assert.isTrue(logCaptor.getValue().contains(expectedError), "Log message should contain the expected error");
    }
}

import org.junit.jupiter.api.Test;
import org.junit.jupiter.api.extension.ExtendWith;
import org.mockito.Mock;
import org.mockito.Mockito;
import org.mockito.junit.jupiter.MockitoExtension;
import org.springframework.http.HttpEntity;
import org.springframework.http.HttpStatus;
import org.springframework.http.ResponseEntity;
import org.springframework.web.client.RestTemplate;
import org.springframework.web.util.UriComponentsBuilder;

import static org.mockito.ArgumentMatchers.any;
import static org.mockito.Mockito.verify;
import static org.mockito.Mockito.when;

@ExtendWith(MockitoExtension.class)
public class FlinkApiControllerTest {

    @Mock
    private RestTemplate restTemplate;

    @Mock
    private KafkaConsumer kafkaConsumer;

    @Mock
    private KafkaInputProducer kafkaInputProducer;

    @Mock
    private HttpHeaders headers;

    @Mock
    private ResponseEntity<String> responseEntity;

    @Test
    public void testTriggerJobSuccess() {
        // Arrange
        FlinkApiController flinkApiController = new FlinkApiController();
        flinkApiController.setFlinkApiUrl("http://localhost:8081");

        SpaceCollector collector = new SpaceCollector();
        String fileType = ".csv";

        UriComponentsBuilder uriBuilder = UriComponentsBuilder.fromHttpUrl("http://localhost:8081/jars/123/run");

        // Set up the mock response
        when(responseEntity.getStatusCode()).thenReturn(HttpStatus.OK);

        // Set up the mock RestTemplate
        when(restTemplate.postForEntity(any(), any(HttpEntity.class), any())).thenReturn(responseEntity);

        // Act
        String result = flinkApiController.triggerJob(collector, fileType);

        // Assert
        Assert.isNull(result, "Job should be triggered successfully");
        verify(restTemplate).postForEntity(uriBuilder.toUriString(), any(HttpEntity.class), any());
    }

    @Test
    public void testTriggerJobFailure() {
        // Arrange
        FlinkApiController flinkApiController = new FlinkApiController();
        flinkApiController.setFlinkApiUrl("http://localhost:8081");

        SpaceCollector collector = new SpaceCollector();
        String fileType = ".csv";

        UriComponentsBuilder uriBuilder = UriComponentsBuilder.fromHttpUrl("http://localhost:8081/jars/123/run");

        // Set up the mock response
        when(responseEntity.getStatusCode()).thenReturn(HttpStatus.INTERNAL_SERVER_ERROR);

        // Set up the mock RestTemplate
        when(restTemplate.postForEntity(any(), any(HttpEntity.class), any())).thenReturn(responseEntity);

        // Act
        String result = flinkApiController.triggerJob(collector, fileType);

        // Assert
        Assert.isTrue(result.equals("Job is not triggered"), "Job should not be triggered");
        verify(restTemplate).postForEntity(uriBuilder.toUriString(), any(HttpEntity.class), any());
    }
}

import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.common.serialization.StringDeserializer;
import org.junit.jupiter.api.AfterEach;
import org.junit.jupiter.api.BeforeEach;
import org.junit.jupiter.api.Test;
import org.junit.jupiter.api.extension.ExtendWith;
import org.mockito.ArgumentCaptor;
import org.mockito.Captor;
import org.mockito.Mock;
import org.mockito.Mockito;
import org.mockito.junit.jupiter.MockitoExtension;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.boot.test.autoconfigure.web.client.RestClientTest;
import org.springframework.kafka.core.KafkaTemplate;
import org.springframework.kafka.support.Acknowledgment;
import org.springframework.kafka.support.KafkaHeaders;
import org.springframework.kafka.support.converter.StringJsonMessageConverter;
import org.springframework.messaging.Message;
import org.springframework.messaging.MessagingException;
import org.springframework.messaging.handler.annotation.Header;
import org.springframework.messaging.handler.annotation.Payload;
import org.springframework.test.context.TestPropertySource;

import java.io.File;
import java.io.IOException;
import java.io.PrintWriter;
import java.nio.file.Files;
import java.nio.file.Paths;
import java.util.*;

import static org.mockito.ArgumentMatchers.*;
import static org.mockito.Mockito.verify;
import static org.mockito.Mockito.when;
import static org.springframework.kafka.support.KafkaHeaders.*;
import static org.springframework.kafka.support.KafkaHeaders.RECEIVED_TOPIC;

@ExtendWith(MockitoExtension.class)
public class KafkaConsumerTest {

    @Autowired
    private StringJsonMessageConverter jsonMessageConverter;

    @Autowired
    private KafkaTemplate<String, Object> kafkaTemplate;

    @Mock
    private FlinkApiController flinkApiController;

    @Mock
    private KafkaInputProducer kafkaInputProducer;

    @Mock
    private FlinkApiController flinkApiController;

    @Mock
    private KafkaAuditProducer kafkaAuditProducer;

    @Mock
    private KafkaConsumer kafkaConsumer;

    @Captor
    private ArgumentCaptor<FileMetadata> metadataCaptor;

    @BeforeEach
    public void setup() {
        // Set up the Kafka consumer properties
        Map<String, Object> consumerProps = new LinkedHashMap<>();
        consumerProps.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, embeddedKafka.getBrokersAsString());
        consumerProps.put(ConsumerConfig.GROUP_ID_CONFIG, "test-group");
        consumerProps.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");

        // Set up the Kafka consumer
        KafkaConsumer<String, String> kafkaConsumer = new KafkaConsumer<>(consumerProps, new StringDeserializer(), new StringDeserializer());

        // Create the topic
        kafkaConsumer.createTopics(TopicBuilder.name("test-topic").partitions(1).build());

        // Create the consumer record
        ConsumerRecord<String, String> consumerRecord = new ConsumerRecord<>("test-topic", 0, 0L, null, "{\"inputFilePath\":\"test-file.csv\"}");

        // Convert the consumer record to a Kafka message
        Message<String> kafkaMessage = jsonMessageConverter.toMessage(consumerRecord, null, null, null);

        // Act
        kafkaConsumerResultListener.onMessage(kafkaMessage, null);
    }

    @AfterEach
    public void tearDown() {
        // Delete the topic
        embeddedKafka.deleteTopics("test-topic");
    }

    @Test
    public void testConsumeSuccess() throws IOException {
        // Arrange
        String sourceFilePath = "test-file.csv";
        String destinationFolderPath = "test-destination";
        String fileType = ".csv";

        SpaceCollector collector = new SpaceCollector();
        collector.setInputFilePath(sourceFilePath);

        FileMetadata metadata = new FileMetadata();
        metadata.setFile_name(sourceFilePath);

        // Set up the mock FlinkApiController
        when(flinkApiController.triggerJob(any(), eq(fileType))).thenReturn("Job triggered successfully");

        // Set up the mock Files
        File sourceFile = new File(sourceFilePath);
        File destinationFolder = new File(destinationFolderPath);
        File logFile = new File(destinationFolderPath + "/log.txt");
        Files.createDirectories(Paths.get(destinationFolderPath));
        sourceFile.createNewFile();

        // Mock the response from Flink API
        ObjectMapper objectMapper = new ObjectMapper();
        String flinkResponse = objectMapper.writeValueAsString(metadata);

        // Set up the mock RestTemplate
        RestTemplate restTemplate = new RestTemplate();
        restTemplate.postForEntity(anyString(), any(HttpEntity.class), String.class)
                .thenReturn(ResponseEntity.ok(flinkResponse));

        // Create the Kafka message
        ConsumerRecord<String, String> consumerRecord = new ConsumerRecord<>("test-topic", 0, 0L, null, "{\"inputFilePath\":\"test-file.csv\"}");

        // Convert the Kafka message to a Kafka consumer record
        Message<String> kafkaMessage = jsonMessageConverter.toMessage(consumerRecord, null, null, null);

        // Act
        kafkaConsumer.consume(kafkaMessage.getPayload());

        // Assert
        verify(flinkApiController).triggerJob(any(), eq(fileType));
        verify(kafkaInputProducer).sendFailureMessage(any(), anyString(), any());
        verify(kafkaInputProducer).writeLogToFile(metadataCaptor.capture(), eq(destinationFolderPath));

        FileMetadata capturedMetadata = metadataCaptor.getValue();
        Assert.notNull(capturedMetadata, "Metadata should not be null");
        Assert.isTrue(capturedMetadata.getFile_name().equals(sourceFilePath), "File name should match");
    }

    @Test
    public void testConsumeFailure_JobTrigger() throws IOException {
        // Arrange
        String sourceFilePath = "test-file.csv";
        String destinationFolderPath = "test-destination";
        String fileType = ".csv";

        SpaceCollector collector = new SpaceCollector();
        collector.setInputFilePath(sourceFilePath);

        FileMetadata metadata = new FileMetadata();
        metadata.setFile_name(sourceFilePath);

        // Set up the mock FlinkApiController
        when(flinkApiController.triggerJob(any(), eq(fileType))).thenReturn("Job failed to trigger");

        // Set up the mock Files
        File sourceFile = new File(sourceFilePath);
        File destinationFolder = new File(destinationFolderPath);
        File logFile = new File(destinationFolderPath + "/log.txt");
        Files.createDirectories(Paths.get(destinationFolderPath));
        sourceFile.createNewFile();

        // Mock the response from Flink API
        ObjectMapper objectMapper = new ObjectMapper();
        String flinkResponse = objectMapper.writeValueAsString(metadata);

        // Set up the mock RestTemplate
        RestTemplate restTemplate = new RestTemplate();
        restTemplate.postForEntity(anyString(), any(HttpEntity.class), String.class)
                .thenReturn(ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR).body("Failed to trigger job"));

        // Create the Kafka message
        ConsumerRecord<String, String> consumerRecord = new ConsumerRecord<>("test-topic", 0, 0L, null, "{\"inputFilePath\":\"