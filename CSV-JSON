ChatGPT & Midjourney | AI bot:
FileMetadata\.java:

```java
public class FileMetadata {
    private String file_name;
    private String date_received;
    private String date_processed;
    private int no_of_records_in_the_file;
    private int no_of_records_processed;
    private int no_of_records_failed;
    private String job_status;
    private String job_id;
    private String jar_id;
    private String exceptions;

    public FileMetadata(String file_name, String date_received, String date_processed, int no_of_records_in_the_file,
                        int no_of_records_processed, int no_of_records_failed, String job_status, String job_id,
                        String jar_id, String exceptions) {
        this.file_name = file_name;
        this.date_received = date_received;
        this.date_processed = date_processed;
        this.no_of_records_in_the_file = no_of_records_in_the_file;
        this.no_of_records_processed = no_of_records_processed;
        this.no_of_records_failed = no_of_records_failed;
        this.job_status = job_status;
        this.job_id = job_id;
        this.jar_id = jar_id;
        this.exceptions = exceptions;
    }

    public String getFile_name() {
        return file_name;
    }

    public void setFile_name(String file_name) {
        this.file_name = file_name;
    }

    public String getDate_received() {
        return date_received;
    }

    public void setDate_received(String date_received) {
        this.date_received = date_received;
    }

    public String getDate_processed() {
        return date_processed;
    }

    public void setDate_processed(String date_processed) {
        this.date_processed = date_processed;
    }

    public int getNo_of_records_in_the_file() {
        return no_of_records_in_the_file;
    }

    public void setNo_of_records_in_the_file(int no_of_records_in_the_file) {
        this.no_of_records_in_the_file = no_of_records_in_the_file;
    }

    public int getNo_of_records_processed() {
        return no_of_records_processed;
    }

    public void setNo_of_records_processed(int no_of_records_processed) {
        this.no_of_records_processed = no_of_records_processed;
    }

    public int getNo_of_records_failed() {
        return no_of_records_failed;
    }

    public void setNo_of_records_failed(int no_of_records_failed) {
        this.no_of_records_failed = no_of_records_failed;
    }

    public String getJob_status() {
        return job_status;
    }

    public void setJob_status(String job_status) {
        this.job_status = job_status;
    }

    public String getJob_id() {
        return job_id;
    }

    public void setJob_id(String job_id) {
        this.job_id = job_id;
    }

    public String getJar_id() {
        return jar_id;
    }

    public void setJar_id(String jar_id) {
        this.jar_id = jar_id;
    }

    public String getExceptions() {
        return exceptions;
    }

    public void setExceptions(String exceptions) {
        this.exceptions = exceptions;
    }
}
```

SpaceCollector\.java:

```java
public class SpaceCollector {

    private static final Logger LOGGER = LogManager.getLogger(SpaceCollector.class);
    private static final String DESTINATION_FOLDER_PATH = "/shared/location/";

    public static void main(String[] args) throws Exception {

        // Initialize Kafka Consumer
        KafkaConsumer kafkaConsumer = new KafkaConsumer();
        kafkaConsumer.init();

        // Initialize Kafka Producer
        KafkaProducer kafkaProducer = new KafkaProducer();

        // Initialize Flink API Controller
        FlinkApiController flinkApiController = new FlinkApiController();

        while (true) {
            try {
                // Consume message from Kafka Topic
                ConsumerRecords<String, String> records = kafkaConsumer.consume();

                for (ConsumerRecord<String, String> record : records) {
                    LOGGER.info("Received message: " + record.value());

                    // Parse the message to get file details
                    FileMetadata fileMetadata = JsonUtils.pa

rseFileMetadata(record.value());

                    // Submit job to Flink API
                    String jobId = flinkApiController.submitJob(fileMetadata.getFile_name(), fileMetadata.getJar_id());

                    // Wait for job to complete
                    String jobStatus = flinkApiController.waitForJobCompletion(jobId);

                    // Check job status
                    if (jobStatus.equals("FINISHED")) {
                        LOGGER.info("Job " + jobId + " completed successfully");

                        // Publish success message to Kafka Topic
                        kafkaProducer.produceSuccessMessage(fileMetadata.getFile_name(), jobId);
                    } else {
                        LOGGER.error("Job " + jobId + " failed");

                        // Publish failure message to Kafka Topic
                        kafkaProducer.produceFailureMessage(fileMetadata, jobId);

                        // Write failure message to log file
                        String logMessage = "Failure message: " + JsonUtils.toJsonString(fileMetadata);
                        FileUtils.writeToFile(DESTINATION_FOLDER_PATH + "failure.log", logMessage);
                    }
                }
            } catch (Exception e) {
                LOGGER.error("Error occurred while processing message: " + e.getMessage());

                // Write error message to log file
                String logMessage = "Error occurred while processing message: " + e.getMessage();
                FileUtils.writeToFile(DESTINATION_FOLDER_PATH + "error.log", logMessage);
            }
        }
    }
}
```

JsonUtils\.java:

```java
public class JsonUtils {

    private static final Gson gson = new Gson();

    public static FileMetadata parseFileMetadata(String json) {
        return gson.fromJson(json, FileMetadata.class);
    }

    public static String toJsonString(FileMetadata fileMetadata) {
        return gson.toJson(fileMetadata);
    }
}
```

KafkaProducer\.java:

```java
public class KafkaProducer {

    private static final Logger LOGGER = LogManager.getLogger(KafkaProducer.class);
    private static final String KAFKA_TOPIC_SUCCESS = "3459-COLLECTION-SUCCESS-QUEUE";
    private static final String KAFKA_TOPIC_FAILURE = "3459-COLLECTION-AUDIT-QUEUE";

    private final KafkaController kafkaController = new KafkaController();

    public void produceSuccessMessage(String fileName, String jobId) {
        String message = "File " + fileName + " processed successfully with job id " + jobId;
        kafkaController.produceMessage(KAFKA_TOPIC_SUCCESS, message);
        LOGGER.info("Published success message to Kafka Topic: " + message);
    }

    public void produceFailureMessage(FileMetadata fileMetadata, String jobId) {
        fileMetadata.setJob_status("FAILED");
        fileMetadata.setJob_id(jobId);

        String message = JsonUtils.toJsonString(fileMetadata);
        kafkaController.produceMessage(KAFKA_TOPIC_FAILURE, message);
        LOGGER.info("Published failure message to Kafka Topic: " + message);
    }
}
```

KafkaController\.java:

```java
public class KafkaController {

    private static final Logger LOGGER = LogManager.getLogger(KafkaController.class);

    private static final String BOOTSTRAP_SERVERS = "localhost:9092";

    private Producer<String, String> producer;

    public void init() {
        Properties props = new Properties();
        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, BOOTSTRAP_SERVERS);
        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());

        producer = new KafkaProducer<>(props);
    }

    public void produceMessage(String topic, String message) {
        ProducerRecord<String, String> record = new ProducerRecord<>(topic, message);
        producer.send(record, new Callback() {
            @Override
            public void onCompletion(RecordMetadata metadata, Exception exception) {

if (exception != null) {
                    LOGGER.error("Error occurred while publishing message to Kafka Topic: " + exception.getMessage());
                } else {
                    LOGGER.info("Message published to Kafka Topic: " + metadata.topic() + ", partition: "
                            + metadata.partition() + ", offset: " + metadata.offset());
                }
            }
        });
    }

    public void close() {
        producer.close();
    }
}
```

FlinkApiController\.java:

```java
public class FlinkApiController {

    private static final Logger LOGGER = LogManager.getLogger(FlinkApiController.class);
    private static final String FLINK_API_URL = "http://localhost:8081";
    private static final String FLINK_JOBS_API = "/jobs";
    private static final String FLINK_JOBS_OVERVIEW_API = "/jobs/overview";

    private static final int MAX_RETRY_COUNT = 10;
    private static final int RETRY_INTERVAL_MS = 5000;

    private final HttpClient httpClient = HttpClient.newHttpClient();

    public String submitJob(String fileName, String jarId) throws Exception {
        String jobName = "Job for file " + fileName;

        // Build request body
        JsonObject requestBody = new JsonObject();
        requestBody.addProperty("entryClass", "com.example.flink.TransformJob");
        requestBody.addProperty("programArgs", "--input-topic 3459-COLLECTION-REQUEST-QUEUE --output-topic 3459-COLLECTION-SUCCESS-QUEUE --file-name " + fileName);
        requestBody.addProperty("parallelism", 1);
        requestBody.addProperty("programJarId", jarId);
        requestBody.addProperty("savepointPath", "");
        requestBody.addProperty("allowNonRestoredState", true);

        // Build request
        HttpRequest request = HttpRequest.newBuilder()
                .uri(URI.create(FLINK_API_URL + FLINK_JOBS_API))
                .header("Content-Type", "application/json")
                .POST(HttpRequest.BodyPublishers.ofString(requestBody.toString()))
                .build();

        // Send request and get response
        HttpResponse<String> response = sendRequestWithRetry(request);

        // Parse response to get job id
        JsonObject responseBody = JsonParser.parseString(response.body()).getAsJsonObject();
        String jobId = responseBody.get("jobid").getAsString();

        LOGGER.info("Submitted job with id " + jobId);

        return jobId;
    }

    public String waitForJobCompletion(String jobId) throws Exception {
        String jobStatus = "";

        // Build request
        HttpRequest request = HttpRequest.newBuilder()
                .uri(URI.create(FLINK_API_URL + FLINK_JOBS_OVERVIEW_API))
                .header("Content-Type", "application/json")
                .GET()
                .build();

        // Wait for job to complete
        while (!jobStatus.equals("FINISHED") && !jobStatus.equals("FAILED")) {
            // Send request and get response
            HttpResponse<String> response = sendRequestWithRetry(request);

            // Parse response to get job status
            JsonObject responseBody = JsonParser.parseString(response.body()).getAsJsonObject();
            JsonArray jobs = responseBody.get("jobs").getAsJsonArray();
            for (JsonElement job : jobs) {
                JsonObject jobObject = job.getAsJsonObject();
                if (jobObject.get("jid").getAsString().equals(jobId)) {
                    jobStatus = jobObject.get("state").getAsString();
                    break;
                }
            }

            Thread.sleep(5000);
        }

        return jobStatus;
    }

    private HttpResponse<String> sendRequestWithRetry(HttpRequest request) throws Exception {
        int retryCount = 0;
        while (retryCount < MAX_RETRY_COUNT) {
            try {
                HttpResponse<String> response = httpClient.send(request, HttpResponse.BodyHandlers.ofString());
                if (response.statusCode() != 200) {
                    throw new Exception("Error occurred while sending request to Fl

ink API: " + response.body());
                }
                return response;
            } catch (Exception e) {
                LOGGER.error("Error occurred while sending request to Flink API: " + e.getMessage());
                retryCount++;
                Thread.sleep(RETRY_INTERVAL_MS);
            }
        }
        throw new Exception("Failed to send request to Flink API after " + MAX_RETRY_COUNT + " retries");
    }
}
```

KafkaConsumer\.java:

```java
public class KafkaConsumer {

    private static final Logger LOGGER = LogManager.getLogger(KafkaConsumer.class);

    private static final String BOOTSTRAP_SERVERS = "localhost:9092";
    private static final String GROUP_ID = "group-id";
    private static final String KAFKA_TOPIC_REQUEST = "3459-COLLECTION-REQUEST-QUEUE";

    private Consumer<String, String> consumer;

    public void init() {
        Properties props = new Properties();
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, BOOTSTRAP_SERVERS);
        props.put(ConsumerConfig.GROUP_ID_CONFIG, GROUP_ID);
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());

        consumer = new KafkaConsumer<>(props);
        consumer.subscribe(Collections.singletonList(KAFKA_TOPIC_REQUEST));
    }

    public ConsumerRecords<String, String> consume() {
        ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(100));
        LOGGER.info("Received " + records.count() + " messages from Kafka Topic");
        return records;
    }

    public void close() {
        consumer.close();
    }
}
```