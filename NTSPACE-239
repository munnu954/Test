Here is the modified Main class with the added functionality to handle Gigamon files using a switch statement:

```java
import org.apache.flink.api.common.functions.FlatMapFunction;
import org.apache.flink.api.java.utils.ParameterTool;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.api.functions.sink.SinkFunction;
import org.apache.flink.streaming.api.functions.source.FileSource;
import org.apache.flink.streaming.api.watermark.WatermarkStrategy;
import org.apache.flink.streaming.connectors.kafka.KafkaRecordSerializationSchema;
import org.apache.flink.streaming.connectors.kafka.KafkaSink;
import org.apache.flink.streaming.util.serialization.SimpleStringSchema;
import org.apache.flink.util.Collector;
import org.apache.flink.core.fs.Path;
import org.apache.flink.formats.textline.TextLineFormat.InputFormat;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.io.File;
import java.io.FileNotFoundException;

public class Main {

    private static String bootstarpServer;
    private static String topic;
    private static final Logger LOGGER = LoggerFactory.getLogger(Main.class);

    public static void main(String[] args) throws Exception {
        ParameterTool parameters = ParameterTool.fromArgs(args);

        bootstarpServer = parameters.get("bootstarpServer");
        topic = parameters.get("Topic");

        LOGGER.info("Start to read and pick the path");

        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        String path = parameters.getRequired("input");
        if (!new File(path).exists()) {
            throw new FileNotFoundException("File not found!");
        }

        final FileSource<String> source = FileSource.forRecordStreamFormat(new InputFormat(), new Path(path)).build();
        final DataStream<String> stream = env.fromSource(source, WatermarkStrategy.noWatermarks(), "file-source");

        KafkaSink<String> sink = KafkaSink.<String>builder()
                .setBootstrapServers(bootstarpServer)
                .setRecordSerializer(KafkaRecordSerializationSchema.builder()
                        .setTopic(topic)
                        .setValueSerializationSchema(new SimpleStringSchema())
                        .build())
                .setDeliveryGuarantee(DeliveryGuarantee.AT_LEAST_ONCE)
                .build();

        final DataStream<String> transformedStream;
        String fileType = parameters.get("fileType");

        switch (fileType) {
            case "Gigamon":
                transformedStream = stream.flatMap(new CsvtoJsonGigamonTransformer());
                break;
            default:
                transformedStream = stream.flatMap(new CsvtoJsonTransformer());
                break;
        }

        transformedStream.sinkTo(sink);
        env.execute();
    }

    public static class CsvtoJsonTransformer implements FlatMapFunction<String, String> {

        @Override
        public void flatMap(String s, Collector<String> collector) throws Exception {
            // Implement your CSV to JSON transformation logic here
        }
    }

    public static class CsvtoJsonGigamonTransformer implements FlatMapFunction<String, String> {

        private String[] headers;

        @Override
        public void flatMap(String s, Collector<String> collector) throws Exception {
            // Implement your Gigamon CSV to JSON transformation logic here
        }

        public void enrichGigamonFlatMap(String s, Collector<String> collector) throws Exception {
            // Implement your Gigamon CSV to JSON transformation logic here
        }
    }
}
```

In this code, the main logic for transforming a CSV record to JSON is implemented in the `CsvtoJsonTransformer` class. For the Gigamon files, you can add a separate `CsvtoJsonGigamonTransformer` class and implement the Gigamon-specific logic in that class. Then, in the main method, a switch statement is used to choose the appropriate transformer based on the `fileType` parameter.