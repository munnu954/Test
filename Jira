Sure, I'll add the constructor method for the FlinkApiController class. Here is the updated code:

```java
@Service
public class FlinkApiController {

    private RestTemplate restTemplate;
    private KafkaConsumer kafkaConsumer;
    private KafkaInputProducer kafkaInputProducer;

    @Value("${flink.api.url}")
    private String flinkApiUrl;

    @Value("${flink.job.csv.jarid}")
    private String flinkJobJarid;

    @Value("${flink.job.csv.program-args)")
    private String programArgs;

    @Value("${flink.job.txt.jarid}")
    private String flinkJobtxtJarid;

    @Value("${flink.job.xml.program-args)")
    private String programXmlArgs;

    @Value("${flink.job.xml.jarid}")
    private String flinkJobXmlJarid;

    private HttpHeaders headers = new HttpHeaders();

    private static final Logger LOGGER = LoggerFactory.getLogger(FlinkApiController.class);

    public FlinkApiController(RestTemplate restTemplate) {
        this.restTemplate = restTemplate;
        this.kafkaConsumer = new KafkaConsumer();
    }

    @Autowired
    public void setKafkaInputProducer(KafkaInputProducer kafkaInputProducer) {
        this.kafkaInputProducer = kafkaInputProducer;
    }

    @PostMapping
    public String triggerJob(SpaceCollector collector, String fileType) {
        LOGGER.info("TRIGGER JOB::::");
        HttpEntity<String> request = null;
        String jobSubmitUrl = null;
        if (programArgs != null && !programArgs.isEmpty()) {
            headers.setContentType(MediaType.APPLICATION_JSON);

            JSONObject requestBody = new JSONObject();
            try {
                if (fileType.equalsIgnoreCase(".csv")) {
                    requestBody.put("programArgs", programArgs.replace("input", collector.getInputFilePath()));
                    jobSubmitUrl = flinkApiUrl + "/jars/" + flinkJobJarid + "/run";
                } else if (fileType.equalsIgnoreCase(".xml")) {
                    LOGGER.info("XML FLINK TRIGGER::");
                    programXmlArgs = programXmlArgs.replace("|filePath|", collector.getInputFilePath());
                    programXmlArgs = programXmlArgs.replace(" |primaryKey|", "");
                    requestBody.put("programArgs", programXmlArgs);
                    jobSubmitUrl = flinkApiUrl + "/jars/" + flinkJobXmlJarid + "/run";
                    LOGGER.info("jobSubmitUrl:" + jobSubmitUrl);
                    LOGGER.info("requestBody::" + requestBody.toString());
                } else if (fileType.equalsIgnoreCase(".txt")) {
                    LOGGER.info("TEXT FLINK TRIGGER::");
                    requestBody.put("programArgs", programArgs.replace("|input|", collector.getInputFilePath()));
                    jobSubmitUrl = flinkApiUrl + "/jars/" + flinkJobtxtJarid + "/run";
                }

                request = new HttpEntity<String>(requestBody.toString(), headers);

                // Trigger the submitted jar
                ResponseEntity<String> response = restTemplate.postForEntity(jobSubmitUrl, request, String.class);
                LOGGER.info("response: {}" + response);

                if (response != null && response.getStatusCode() != HttpStatus.OK) {
                    LOGGER.info("Not Ok!!");
                    FileMetadata metadata = kafkaConsumer.logMetadata(collector, fileType, response);
                    kafkaInputProducer.sendFailureMessage(collector, fileType, response);
                    kafkaConsumer.writeLogToFile(collector, fileType, response, destinationFolderPath);
                }
            } catch (HttpClientErrorException | HttpServerErrorException ex) {
                ex.printStackTrace();
                LOGGER.error("Error triggering job", ex.getMessage());
                FileMetadata metadata = kafkaConsumer.logMetadata(collector, fileType, ex);
                kafkaInputProducer.sendFailureMessage(collector, fileType, ex);
                kafkaConsumer.writeLogToFile(metadata, destinationFolderPath);
            } catch (Exception e) {
                e.printStackTrace();
                LOGGER.error("Error triggering job", e.getMessage());
                FileMetadata metadata = kafkaConsumer.logMetadata(collector, fileType, e);
                kafkaInputProducer.sendFailureMessage(collector, fileType, e);
                kafkaConsumer.writeLogToFile(metadata, destinationFolderPath);
            }
        }
        return "Job is not trigged";
    }
}
```

Now you can create an instance of the FlinkApiController class with the `RestTemplate` as a constructor parameter.