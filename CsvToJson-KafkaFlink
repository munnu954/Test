
Packages and Classes to be created:

1. Package: com.example.customtextdatatransformer.flink
- This package will contain all the classes related to Flink.

2. Class: TextToJsonTransformerJob
- This class will contain the Flink job logic for transforming text data to JSON format.

3. Class: TextToJsonTransformerTest
- This class will contain the test logic for validating the generated JSON.

4. Class: FlinkJobUploader
- This class will contain the logic for uploading the Flink job to the cluster.

Complete code for the project:

1. TextToJsonTransformerJob.java

```
package com.example.customtextdatatransformer.flink;

import org.apache.flink.api.common.functions.FlatMapFunction;
import org.apache.flink.api.common.functions.MapFunction;
import org.apache.flink.api.common.serialization.SimpleStringEncoder;
import org.apache.flink.api.common.serialization.SimpleStringSchema;
import org.apache.flink.api.common.typeinfo.Types;
import org.apache.flink.core.fs.Path;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.api.functions.sink.filesystem.StreamingFileSink;
import org.apache.flink.streaming.api.functions.sink.filesystem.bucketassigners.DateTimeBucketAssigner;
import org.apache.flink.streaming.api.windowing.time.Time;
import org.apache.flink.util.Collector;

import java.time.ZoneOffset;
import java.time.format.DateTimeFormatter;

public class TextToJsonTransformerJob {

public static void main(String[] args) throws Exception {
final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

DataStream textData = env.socketTextStream("localhost", 9000);

DataStream jsonData = textData.flatMap(new FlatMapFunction() {
@Override
public void flatMap(String value, Collector out) throws Exception {
String[] words = value.split(" ");
for (String word : words) {
out.collect("{\"word\": \"" + word + "\", \"count\": 1}");
}
}
}).returns(Types.STRING)
.keyBy(json -> 1)
.timeWindow(Time.seconds(10))
.sum("count")
.map(new MapFunction() {
@Override
public String map(String value) throws Exception {
return value.replaceAll("\\\\", "");
}
});

DateTimeFormatter formatter = DateTimeFormatter.ofPattern("yyyy-MM-dd-HH-mm-ss").withZone(ZoneOffset.UTC);

StreamingFileSink sink = StreamingFileSink.forRowFormat(new Path("/path/to/output"), new SimpleStringEncoder())
.withBucketAssigner(new DateTimeBucketAssigner.withBucketCheckInterval(1000)
.withRollingPolicy(
org.apache.flink.streaming.api.functions.sink.filesystem.RollingPolicy
.create()
.withRolloverInterval(Time.seconds(5))
.withInactivityInterval(Time.seconds(2))
.withMaxPartSize(1024 * 1024 * 1024)
).build();

jsonData.addSink(sink);

env.execute("Text to JSON Transformer Job");
}
}
```

2. TextToJsonTransformerTest.java

```
package com.example.customtextdatatransformer.flink;

import org.apache.flink.api.common.functions.FlatMapFunction;
import org.apache.flink.api.common.functions.MapFunction;
import org.apache.flink.api.common.serialization.SimpleStringEncoder;
import org.apache.flink.api.common.serialization.SimpleStringSchema;
import org.apache.flink.api.common.typeinfo.Types;
import org.apache.flink.core.fs.Path;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.api.functions.sink.filesystem.StreamingFileSink;
import org.apache.flink.streaming.api.functions.sink.filesystem.bucketassigners.DateTimeBucketAssigner;
import org.apache.flink.streaming.api.windowing.time.Time;
import org.apache.flink.util.Collector;
import org.junit.Test;

import java.time.ZoneOffset;
import java.time.format.DateTimeFormatter;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.List;

import static org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.getExecutionEnvironment;
import static org.junit.Assert.assertEquals;

public class TextToJsonTransformerTest {

@Test
public void testTextToJsonTransformerJob() throws Exception {
StreamExecutionEnvironment env = getExecutionEnvironment();

List input = new ArrayList
DataStream textData = env.fromCollection(input);

DataStream jsonData = textData.flatMap(new FlatMapFunction() {
@Override
public void flatMap(String value, Collector out) throws Exception {
String[] words = value.split(" ");
for (String word : words) {
out.collect("{\"word\": \"" + word + "\", \"count\": 1}");
}
}
}).returns(Types.STRING)
.keyBy(json -> 1)
.timeWindow(Time.seconds(10))
.sum("count")
.map(new MapFunction() {
@Override
public String map(String value) throws Exception {
return value.replaceAll("\\\\", "");
}
});

DateTimeFormatter formatter = DateTimeFormatter.ofPattern("yyyy-MM-dd-HH-mm-ss").withZone(ZoneOffset.UTC);

StreamingFileSink sink = StreamingFileSink.forRowFormat(new Path("/path/to/output"), new SimpleStringEncoder())
.withBucketAssigner(new DateTimeBucketAssigner.withBucketCheckInterval(1000)
.withRollingPolicy(
org.apache.flink.streaming.api.functions.sink.filesystem.RollingPolicy
.create()
.withRolloverInterval(Time.seconds(5))
.withInactivityInterval(Time.seconds(2))
.withMaxPartSize(1024 * 1024 * 1024)
).build();

jsonData.addSink(sink);

jsonData.print();

env.execute("Text to JSON Transformer Job");

// Validate generated JSON
List expectedOutput = new ArrayList
assertEquals(expectedOutput, jsonData.collect());
}
}
```

3. FlinkJobUploader.java

```
package com.example.customtextdatatransformer.flink;

import org.apache.flink.client.program.PackagedProgram;
import org.apache.flink.client.program.PackagedProgramUtils;
import org.apache.flink.configuration.Configuration;
import org.apache.flink.configuration.JobManagerOptions;
import org.apache.flink.configuration.RestOptions;
import org.apache.flink.runtime.jobgraph.JobGraph;
import org.apache.flink.yarn.YarnClusterDescriptor;

import java.io.File;

public class FlinkJobUploader {

public static void main(String[] args) throws Exception {
Configuration flinkConfig = new Configuration();
flinkConfig.setString(JobManagerOptions.ADDRESS, "localhost");
flinkConfig.setInteger(RestOptions.PORT, 8081);

YarnClusterDescriptor clusterDescriptor = new YarnClusterDescriptor(flinkConfig, null, null);

clusterDescriptor.setLocalJarPath(new Path("/path/to/job.jar"));

PackagedProgram program = PackagedProgram.newBuilder()
.setJarFile(new File("/path/to/job.jar"))
.setEntryPointClassName("com.example.customtextdatatransformer.flink.TextToJsonTransformerJob")
.setArguments(args)
.build();

JobGraph jobGraph = PackagedProgramUtils.createJobGraph(program, flinkConfig, 1);

clusterDescriptor.deployJobCluster(jobGraph, true);

clusterDescriptor.close();
}
}
```

Note: Replace the `/path/to/job.jar` in the above code with the actual path to the job jar file.