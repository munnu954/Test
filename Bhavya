Toggle Sidebar


FlinkApiController.java:-

@Service public class FlinkApiController {

@Value("${flink.api.url}") private String flinkApiUrl;

@Value("${flink.job.csv.jarid}") private String flinkJobJarid;

@Value("${flink.job.csv.program-args)") private String programArgs;

@Value("${flink.job.txt.jarid}") private String flinkJobtxtJarid;

@Value("${flink.job.xml.program-args)") private String programXmlArgs;

@Value("${flink.job.xml.jarid}") private String flinkJobXmlJarid;

@Value("${log.directory.path)") private String logFolderPath;

private KafkaConsumer KafkaConsumer = new KafkaConsumer (); @Autowired private KafkaAuditProducer kafkaAuditProducer; HttpHeaders headers = new HttpHeaders();

private static final Logger LOGGER = LoggerFactory.getLogger(FlinkApiController.class);

@PostMapping public String trigger Job (UnifiedAuditMessage auditMsg, String fileType) {

RestTemplate restTemplate = new RestTemplate(); LOGGER.info("TRIGGER JOB::::"); HttpEntity<String> request = null; String jobSubmitUrl = null; if(programArgs !=null && !programArgs.isEmpty()){ headers.setContentType(MediaType.APPLICATION_JSON);

JSONObject requestBody = new JSONObject(); try{ if(fileType.equalsIgnoreCase(".csv")) { requestBody.put("programArgs", programArgs.replace("input", auditMsg.getInputFilePath())); jobSubmitUrl = flinkApiUrl + "/jars/" + flinkJobJarid + "/run";

} else if(fileType.equalsIgnoreCase(".xml")){ LOGGER.info("XML FLINK TRIGGER::");

programXmlArgs = programXmlArgs.replace("|filePath|", auditMsg.getInputFilePath()); programXmlArgs = programXmlArgs.replace(" |primaryKey|", ""); requestBody.put("programArgs", programXmlArgs); jobSubmitUrl = flinkApiUrl + "/jars/" + flinkJobXmlJarid + "/run"; LOGGER.info("jobSubmitUrl:" + jobSubmitUrl); LOGGER.info("requestBody::" + requestBody.toString()); } else if(fileType.equalsIgnoreCase(".txt")) {LOGGER.info("TEXT FLINK TRIGGER::");

requestBody.put("programArgs", programArgs.replace("|input|", auditMsg.getInputFilePath()));

jobSubmitUrl = flinkApiUrl + "/jars/" + flinkJobtxtJarid + "/run";

}

request = new HttpEntity<String>(requestBody.toString(), headers); }

//Trigger the submitted jar ResponseEntity<String> response = restTemplate.postForEntity(jobSubmitUrl, request, String.class); LOGGER.info("response: {}"+response.getBody());

if (response != null && response.getStatusCode() ! = HttpStatus.OK) { ObjectMapper objectMapper =

new ObjectMapper();

JsonNode responseBody = objectMapper.readTree(response.getBody()); JsonNode jobIDNode = responseBody.get("jobid");

String jobID= (jobIDNode != null) ? jobIDNode.asText(): null; LOGGER.info("jobID::"+jobID); kafkaAuditProducer.createMessageAndLog(auditMsg.getInputFilePath(), auditMsg.getUrl(), auditMsg.getPort(), auditMsg.getOutputFilePath(), auditMsg.getDelimiters(), fileType, new SimpleDateFormat("yyyy-MM-dd HH:mm:ss").format(new Date()), auditMsg.getNo_of_records_in_the_file(), auditMsg.getNo_of_records_processed(), auditMsg.getNo_of_records_failed(), auditMsg.getJob_id(), auditMsg.getJar_id(), auditMsg.getCollector_id(), auditMsg.getExceptions(), auditMsg.getJobStatus()); update JobStatus (auditMsg, jobID); return "Job submitted"; } else{ LOGGER.info("Not Ok!!"); UnifiedAuditMessage auditData = kafkaConsumer.logMetadata(auditMsg, fileType, response); kafkaAuditProducer.createMessageAndLog(auditMsg.getInputFilePath(), auditMsg.getUrl(), auditMsg.getPort(), auditMsg.getOutputFilePath(), auditMsg.getDelimiters(), fileType, new SimpleDateFormat("yyyy-MM-dd HH:mm:ss").format(new Date()), auditMsg.getNo_of_records_in_the_file(), auditMsg.getNo_of_records_processed(), auditMsg.getNo_of_records_failed(), auditMsg.getJob_id(), auditMsg.getJar_id(), auditMsg.getCollector_id(), auditMsg.getExceptions()); ObjectMapper objectMapper = new ObjectMapper(); JsonNode responseBody = objectMapper.readTree(response.getBody()); JsonNode jobIDNode = responseBody.get("jobid");

S M A, [1/22/24, 10:50 PM] String jobID= (jobIDNode != null) ? jobIDNode.asText(): null; LOGGER.info("jobID::"+jobID); update JobStatus (auditMsg, jobID);

Sweet Candy, [1/23/24, 12:12 PM] kafkaConsumer.writeLogToFile(auditMsg, fileType, response, logFolderPath);

} } catch (Exception e) { e.printStackTrace(); LOGGER.error("Error triggering job", e.getMessage()); UnifiedAuditMessage auditData = kafkaConsumer.logauditData(auditMsg, fileType, e); kafkaAuditProducer.createMessageAndLog(auditMsg.getInputFilePath(), auditMsg.getUrl(), auditMsg.getPort(), auditMsg.getOutputFilePath(), auditMsg.getDelimiters(), fileType, new SimpleDateFormat("yyyy-MM-dd HH:mm:ss").format(new Date()), auditMsg.getNo_of_records_in_the_file(), auditMsg.getNo_of_records_processed(), auditMsg.getNo_of_records_failed(), auditMsg.getJob_id(), auditMsg.getJar_id(), auditMsg.getCollector_id(), auditMsg.getExceptions()); kafkaConsumer.writeLogToFile(auditData, logFolderPath); } } return "Job is not triggered"; } private void updateJobStatus(Unified AuditMessage auditMsg, String jobID) { try { RestTemplate restTemplate = new RestTemplate(); JsonNode jsonNode = restTemplate.getForObject(flinkApiUrl + "/jobs/" + jobID, JsonNode.class); LOGGER.info("jsonNode::"+jsonNode); while (true) { if (jsonNode.get("state").textValue().equals("FINISHED") jsonNode.get("state").textValue().equals("CANCELED") jsonNode.get("state").textValue().equals("FAILED") || jsonNode.get("state").textValue().equals("SUSPENDED")) { break; } Thread.sleep(10); jsonNode restTemplate.getForObject(flinkApiUrl + "/jobs/" + jobID, JsonNode.class); } if (jsonNode.get("state").textValue().equals("FINISHED")) { auditMsg.setJobStatus (JobStatus.FLINK_JOB_SUBMITTED.toString()); } else { auditMsg.setJobStatus (JobStatus.FLINK_JOB_FAILED.toString()); } } catch (Exception e) { LOGGER.error("Error updating job status", e.getMessage()); } } } KafkaAuditProducer.java:- @Service public class KafkaAuditProducer { private static final Logger Logger = LoggerFactory.getLogger(KafkaAuditProducer.class); @Value("${spring.kafka.producer.topic-name}") private String topicName;

Download
Copy code
private final KafkaTemplate<String, Object> kafkaTemplate;

@Autowired
public KafkaAuditProducer(KafkaTemplate<String, Object> kafkaTemplate) {
    this.kafkaTemplate = kafkaTemplate;
}
public String sendMessage(UnifiedAuditMessage auditMsg) { Logger.info(String.format("Message sent: %s", auditMsg)); kafkaTemplate.send(topicName, auditMsg); return "Message sent successfully"; } public void createMessageAndLog(String inputFilePath, String url, Integer port, String outputFilePath, String delimiters, String fileType, String date_received, Integer no_of_records_in_the_file, Integer no_of_records_processed, Integer no_of_records_failed, String job_id, String jar_id, String collector_id, List<String> exceptions, String jobStatus) { LocalDateTime dateProcessed LocalDateTime.now(); String date_processed = dateProcessed.format(DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss.SSS")); Unified AuditMessage auditMsg = new Unified AuditMessage(inputFilePath, collector_id, url, port, outputFilePath, delimiters, fileType, date_received, date_processed, no_of_records_in_the_file, no_of_records_processed, no_of_records_failed, exceptions, job_id, jar_id, jobStatus); try { this.kafkaTemplate.send(topicName, auditMsg); Logger.info("Kafka status message {}", auditMsg); } catch (Exception e) { Logger.error("Failed to send Kafka message: {}", e.getMessage()); } } AC1: From custom-data-transformation-client microservice, Given a Unified Audit Message has been received by the custom-data-transformer-client in 3459-DEV-COLLECTION-AUDIT-QUEUE When the transformation job submision is successful Then We publish the new format of Unified Audit Message to the 3459-DEV-COLLECTION-AUDIT-QUEUE with job_status FLINK_JOB_SUMBITTED

AC2: From custom-data-transformation-client microservice, Given a Unified Audit Message has been received by the custom-data-transformer-client in 3459-DEV-COLLECTION-AUDIT-QUEUE When the transformation job submision is NOT successful Then We publish the new format of Unified Audit Message to the 3459-DEV-COLLECTION-AUDIT-QUEUE with job_status FLINK_JOB_FAILED

Write postive and negative test case code for above acceptance criteria by referring above code


Positive test case for AC1:

java
Download
Copy code
@Test
public void testTriggerJobSuccess() {
    UnifiedAuditMessage auditMsg = new UnifiedAuditMessage();
    String fileType = ".csv";
    String inputFilePath = "/path/to/input/file";
    String programArgs = "input=" + inputFilePath;
    String jobSubmitUrl = "http://flink-api-url/jars/jar-id/run";
    String responseBody = "{\"jobid\": \"123456789\"}";

    when(kafkaConsumer.logMetadata(auditMsg, fileType, null)).thenReturn(auditMsg);
    when(restTemplate.postForEntity(jobSubmitUrl, any(), eq(String.class))).thenReturn(new ResponseEntity<>(responseBody, HttpStatus.OK));

    String result = flinkApiController.triggerJob(auditMsg, fileType);

    verify(kafkaAuditProducer, times(1)).createMessageAndLog(inputFilePath, null, null, null, null, fileType, anyString(), anyInt(), anyInt(), anyInt(), anyString(), anyString(), anyList(), eq(JobStatus.FLINK_JOB_SUBMITTED.toString()));
    assertEquals("Job submitted", result);
}
Negative test case for AC2:

java
Download
Copy code
@Test
public void testTriggerJobFailure() {
    UnifiedAuditMessage auditMsg = new UnifiedAuditMessage();
    String fileType = ".csv";
    String inputFilePath = "/path/to/input/file";
    String programArgs = "input=" + inputFilePath;
    String jobSubmitUrl = "http://flink-api-url/jars/jar-id/run";

    when(kafkaConsumer.logMetadata(auditMsg, fileType, any())).thenReturn(auditMsg);
    when(restTemplate.postForEntity(jobSubmitUrl, any(), eq(String.class))).thenReturn(new ResponseEntity<>("{\"error\": \"An error occurred\"}", HttpStatus.INTERNAL_SERVER_ERROR));

    String result = flinkApiController.triggerJob(auditMsg, fileType);

    verify(kafkaAuditProducer, times(1)).createMessageAndLog(inputFilePath, null, null, null, null, fileType, anyString(), anyInt(), anyInt(), anyInt(), anyString(), anyString(), anyList(), eq(JobStatus.FLINK_JOB_FAILED.toString()));
    assertEquals("Job is not triggered", result);
}
Note: The above test cases are for demonstration purposes only and may need to be adapted to your specific testing environment and framework. The restTemplate and kafkaConsumer mock objects are created using Mockito.

Is this conversation helpful so far?



Not giving any suggestions for eq(JobStatus.FLINK_JOB_FAILED.toString()))


Both the test cases you provided cover different scenarios and can be used as they are. The only suggestion I can give is to consider using parameterized tests for more flexibility and test case reduction.

In your provided code, there is a hardcoded jobSubmitUrl ("http://flink-api-url/jars/jar-id/run") which is used to mock the restTemplate behavior. This may not be ideal as different environments might have different job submit URLs. You can consider using a property placeholder to get the actual URL at runtime.

java
Download
Copy code
@Value("${flink.api.url}")
private String flinkApiUrl;
Remember to replace ${flink.api.url} with the actual property key from your configuration.

You can then use the flinkApiUrl variable in your tests:

java
Download
Copy code
String jobSubmitUrl = flinkApiUrl + "/jars/jar-id/run";
This will allow you to specify different job submit URLs for different environments in your configuration files, which can then be loaded into your tests.

Although your test cases cover both success and failure scenarios, you may want to add additional tests to cover different error scenarios or corner cases that could occur during the execution of your flink job. These additional tests can help ensure the robustness and reliability of your application.

If you need more assistance, please feel free to ask!

Is this conversation helpful so far?



FlinkApiController.java:-

@Service public class FlinkApiController {

@Value("${flink.api.url}") private String flinkApiUrl;

@Value("${flink.job.csv.jarid}") private String flinkJobJarid;

@Value("${flink.job.csv.program-args)") private String programArgs;

@Value("${flink.job.txt.jarid}") private String flinkJobtxtJarid;

@Value("${flink.job.xml.program-args)") private String programXmlArgs;

@Value("${flink.job.xml.jarid}") private String flinkJobXmlJarid;

@Value("${log.directory.path)") private String logFolderPath;

private KafkaConsumer KafkaConsumer = new KafkaConsumer (); @Autowired private KafkaAuditProducer kafkaAuditProducer; HttpHeaders headers = new HttpHeaders();

private static final Logger LOGGER = LoggerFactory.getLogger(FlinkApiController.class);

@PostMapping public String trigger Job (UnifiedAuditMessage auditMsg, String fileType) {

RestTemplate restTemplate = new RestTemplate(); LOGGER.info("TRIGGER JOB::::"); HttpEntity<String> request = null; String jobSubmitUrl = null; if(programArgs !=null && !programArgs.isEmpty()){ headers.setContentType(MediaType.APPLICATION_JSON);

JSONObject requestBody = new JSONObject(); try{ if(fileType.equalsIgnoreCase(".csv")) { requestBody.put("programArgs", programArgs.replace("input", auditMsg.getInputFilePath())); jobSubmitUrl = flinkApiUrl + "/jars/" + flinkJobJarid + "/run";

} else if(fileType.equalsIgnoreCase(".xml")){ LOGGER.info("XML FLINK TRIGGER::");

programXmlArgs = programXmlArgs.replace("|filePath|", auditMsg.getInputFilePath()); programXmlArgs = programXmlArgs.replace(" |primaryKey|", ""); requestBody.put("programArgs", programXmlArgs); jobSubmitUrl = flinkApiUrl + "/jars/" + flinkJobXmlJarid + "/run"; LOGGER.info("jobSubmitUrl:" + jobSubmitUrl); LOGGER.info("requestBody::" + requestBody.toString()); } else if(fileType.equalsIgnoreCase(".txt")) {LOGGER.info("TEXT FLINK TRIGGER::");

requestBody.put("programArgs", programArgs.replace("|input|", auditMsg.getInputFilePath()));

jobSubmitUrl = flinkApiUrl + "/jars/" + flinkJobtxtJarid + "/run";

}

request = new HttpEntity<String>(requestBody.toString(), headers); }

//Trigger the submitted jar ResponseEntity<String> response = restTemplate.postForEntity(jobSubmitUrl, request, String.class); LOGGER.info("response: {}"+response.getBody());

if (response != null && response.getStatusCode() ! = HttpStatus.OK) { ObjectMapper objectMapper =

new ObjectMapper();

JsonNode responseBody = objectMapper.readTree(response.getBody()); JsonNode jobIDNode = responseBody.get("jobid");

String jobID= (jobIDNode != null) ? jobIDNode.asText(): null; LOGGER.info("jobID::"+jobID); kafkaAuditProducer.createMessageAndLog(auditMsg.getInputFilePath(), auditMsg.getUrl(), auditMsg.getPort(), auditMsg.getOutputFilePath(), auditMsg.getDelimiters(), fileType, new SimpleDateFormat("yyyy-MM-dd HH:mm:ss").format(new Date()), auditMsg.getNo_of_records_in_the_file(), auditMsg.getNo_of_records_processed(), auditMsg.getNo_of_records_failed(), auditMsg.getJob_id(), auditMsg.getJar_id(), auditMsg.getCollector_id(), auditMsg.getExceptions(), auditMsg.getJobStatus()); update JobStatus (auditMsg, jobID); return "Job submitted"; } else{ LOGGER.info("Not Ok!!"); UnifiedAuditMessage auditData = kafkaConsumer.logMetadata(auditMsg, fileType, response); kafkaAuditProducer.createMessageAndLog(auditMsg.getInputFilePath(), auditMsg.getUrl(), auditMsg.getPort(), auditMsg.getOutputFilePath(), auditMsg.getDelimiters(), fileType, new SimpleDateFormat("yyyy-MM-dd HH:mm:ss").format(new Date()), auditMsg.getNo_of_records_in_the_file(), auditMsg.getNo_of_records_processed(), auditMsg.getNo_of_records_failed(), auditMsg.getJob_id(), auditMsg.getJar_id(), auditMsg.getCollector_id(), auditMsg.getExceptions()); ObjectMapper objectMapper = new ObjectMapper(); JsonNode responseBody = objectMapper.readTree(response.getBody()); JsonNode jobIDNode = responseBody.get("jobid");

S M A, [1/22/24, 10:50 PM] String jobID= (jobIDNode != null) ? jobIDNode.asText(): null; LOGGER.info("jobID::"+jobID); update JobStatus (auditMsg, jobID);

Sweet Candy, [1/23/24, 12:12 PM] kafkaConsumer.writeLogToFile(auditMsg, fileType, response, logFolderPath);

} } catch (Exception e) { e.printStackTrace(); LOGGER.error("Error triggering job", e.getMessage()); UnifiedAuditMessage auditData = kafkaConsumer.logauditData(auditMsg, fileType, e); kafkaAuditProducer.createMessageAndLog(auditMsg.getInputFilePath(), auditMsg.getUrl(), auditMsg.getPort(), auditMsg.getOutputFilePath(), auditMsg.getDelimiters(), fileType, new SimpleDateFormat("yyyy-MM-dd HH:mm:ss").format(new Date()), auditMsg.getNo_of_records_in_the_file(), auditMsg.getNo_of_records_processed(), auditMsg.getNo_of_records_failed(), auditMsg.getJob_id(), auditMsg.getJar_id(), auditMsg.getCollector_id(), auditMsg.getExceptions()); kafkaConsumer.writeLogToFile(auditData, logFolderPath); } } return "Job is not triggered"; } private void updateJobStatus(Unified AuditMessage auditMsg, String jobID) { try { RestTemplate restTemplate = new RestTemplate(); JsonNode jsonNode = restTemplate.getForObject(flinkApiUrl + "/jobs/" + jobID, JsonNode.class); LOGGER.info("jsonNode::"+jsonNode); while (true) { if (jsonNode.get("state").textValue().equals("FINISHED") jsonNode.get("state").textValue().equals("CANCELED") jsonNode.get("state").textValue().equals("FAILED") || jsonNode.get("state").textValue().equals("SUSPENDED")) { break; } Thread.sleep(10); jsonNode restTemplate.getForObject(flinkApiUrl + "/jobs/" + jobID, JsonNode.class); } if (jsonNode.get("state").textValue().equals("FINISHED")) { auditMsg.setJobStatus (JobStatus.FLINK_JOB_SUBMITTED.toString()); } else { auditMsg.setJobStatus (JobStatus.FLINK_JOB_FAILED.toString()); } } catch (Exception e) { LOGGER.error("Error updating job status", e.getMessage()); } } } KafkaAuditProducer.java:- @Service public class KafkaAuditProducer { private static final Logger Logger = LoggerFactory.getLogger(KafkaAuditProducer.class); @Value("${spring.kafka.producer.topic-name}") private String topicName;

Download
Copy code
private final KafkaTemplate<String, Object> kafkaTemplate;

@Autowired
public KafkaAuditProducer(KafkaTemplate<String, Object> kafkaTemplate) {
    this.kafkaTemplate = kafkaTemplate;
}
public String sendMessage(UnifiedAuditMessage auditMsg) { Logger.info(String.format("Message sent: %s", auditMsg)); kafkaTemplate.send(topicName, auditMsg); return "Message sent successfully"; } public void createMessageAndLog(String inputFilePath, String url, Integer port, String outputFilePath, String delimiters, String fileType, String date_received, Integer no_of_records_in_the_file, Integer no_of_records_processed, Integer no_of_records_failed, String job_id, String jar_id, String collector_id, List<String> exceptions, String jobStatus) { LocalDateTime dateProcessed LocalDateTime.now(); String date_processed = dateProcessed.format(DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss.SSS")); Unified AuditMessage auditMsg = new Unified AuditMessage(inputFilePath, collector_id, url, port, outputFilePath, delimiters, fileType, date_received, date_processed, no_of_records_in_the_file, no_of_records_processed, no_of_records_failed, exceptions, job_id, jar_id, jobStatus); try { this.kafkaTemplate.send(topicName, auditMsg); Logger.info("Kafka status message {}", auditMsg); } catch (Exception e) { Logger.error("Failed to send Kafka message: {}", e.getMessage()); } } KafkaConsumer class:

`java @Service public class KafkaConsumer { private static final Logger LOGGER LoggerFactory.getLogger(KafkaConsumer.class);

Download
Copy code
@Value("${file.txtsource)")
private String sourceFilePath;

@Value("${file.destination)")
private String destinationFolderPath;

@Value("${spring.kafka.producer.topic-name")
private String topic;

@Autowired
private KafkaTopicUtil util;

@Autowired
private FlinkApiController FlinkApi;

@Autowired 
private KafkaAuditProducer kafkaAuditProducer;
 @KafkaListener(topics="${spring.kafka.producer.topic-name}", groupId = "${spring.kafka.consumer.group-id}", errorHandler = "kafkaEventErrorHandler")
public void consume(String data) throws IOException {
    LocalDateTime dateReceived = LocalDateTime.now();
    UnifiedAuditMessage auditMsg = null;
    String fileType = null;
    
    try {
        if(data.contains("inputFilePath")) {
            ObjectMapper mapper = new ObjectMapper();
            auditMsg = mapper.readValue(data, UnifiedAuditMessage.class);
            LOGGER.info("Message Received: {}", auditMsg);
            LOGGER.info("Message received-> {}", auditMsg);
            
            String sourceFilePath = auditMsg.getInputFilePath();
            String response = null;
            
            if((new File(sourceFilePath).exists()) && !util.isKafkaTopicPresent(topic)) {
                UnifiedAuditMessage auditData = new UnifiedAuditMessage();
                auditData.setFile_name(sourceFilePath);
                auditData.setExceptions("Input file path doesn't exist/Invalid file");
                writeLogToFile(auditData, destinationFolderPath);
            }
            
            if(validateFile(sourceFilePath)) {
                writeFile(sourceFilePath, destinationFolderPath);
                String fileType = getExtension(sourceFilePath);
                
                if(fileType != null && fileType.equalsIgnoreCase(".xml")) {
                    LOGGER.info("XML FILE()");
                    retrieveXmlFile(sourceFilePath);
                } else if(fileType != null && fileType.equalsIgnoreCase(".csv")) {
                    LOGGER.info("CSV FILE()");
                    retrieveCsvFile(sourceFilePath);
                } else if(fileType != null && fileType.equalsIgnoreCase(".txt")) {
                    LOGGER.info("TEXT FILE()");
                    retrieveTxtFile(sourceFilePath);
                }
                response = getFlinkResponse();
                LOGGER.info("Flink Response: {}", response);
            } else {
                kafkaAuditProducer.sendErrorMessage("File does not exist");
            }
        }
    } catch (Exception e) {
        e.printStackTrace();
        LOGGER.error("Error triggering job: {}", e.getMessage());
        
        UnifiedAuditMessage auditData = logAuditData(auditMsg, fileType, e);
        kafkaAuditProducer.createMessageAndLog(auditData.getInputFilePath(), auditData.getUrl(), auditData.getPort(), auditData.getOutputFilePath(), auditData.getDelimiters(), auditData.getFileType(), auditData.getDate_received(), auditData.getNo_of_records_in_the_file(), auditData.getNo_of_records_processed(), auditData.getNo_of_records_failed(), auditData.getJob_id(), auditData.getJar_id(), auditData.getExceptions(), auditData.getCollector_id(), auditData.getJob_status());
    }
}

public UnifiedAuditMessage logAuditData(UnifiedAuditMessage auditMsg, String fileType, Exception e) {
    DateTimeFormatter formatter = DateTimeFormatter.ofPattern("yyyy-MM-dd'T'HH:mm:ss");
    LocalDateTime currentDate = LocalDateTime.now();
  
    return new UnifiedAuditMessage(
            auditMsg.getInputFilePath(),
            auditMsg.getUrl(),
            auditMsg.getPort(),
            auditMsg.getOutputFilePath(),
            auditMsg.getDelimiters(),
            fileType,
            currentDate,
            currentDate,
            auditMsg.getNo_of_records_in_the_file(),
            auditMsg.getNo_of_records_processed(),
            auditMsg.getNo_of_records_failed(),
            UUID.randomUUID(),
            UUID.randomUUID(),
            Collections.singletonList(e.getMessage()),
            auditMsg.getCollector_id(),
            UnifiedAuditMessage.JobStatus.COLLECTION_FAILED
    );
}

// Other methods and logAuditData method goes here...
} AC1: From custom-data-transformation-client microservice, Given a Unified Audit Message has been received by the custom-data-transformer-client in 3459-DEV-COLLECTION-AUDIT-QUEUE When the transformation job submision is successful Then We publish the new format of Unified Audit Message to the 3459-DEV-COLLECTION-AUDIT-QUEUE with job_status FLINK_JOB_SUMBITTED

AC2: From custom-data-transformation-client microservice, Given a Unified Audit Message has been received by the custom-data-transformer-client in 3459-DEV-COLLECTION-AUDIT-QUEUE When the transformation job submision is NOT successful Then We publish the new format of Unified Audit Message to the 3459-DEV-COLLECTION-AUDIT-QUEUE with job_status FLINK_JOB_FAILED

Write postive and negative test case code for above acceptance criteria by referring above code


Sure, I can help you with that. Here are the positive and negative test cases for the given acceptance criteria:

AC1: From custom-data-transformation-client microservice, Given a Unified Audit Message has been received by the custom-data-transformer-client in 3459-DEV-COLLECTION-AUDIT-QUEUE When the transformation job submision is successful Then We publish the new format of Unified Audit Message to the 3459-DEV-COLLECTION-AUDIT-QUEUE with job_status FLINK_JOB_SUBMITTED

Positive Test Case:

@RunWith(SpringRunner.class) @SpringBootTest public class FlinkApiControllerTest {

@MockBean private KafkaAuditProducer kafkaAuditProducer;

@Autowired private FlinkApiController flinkApiController;

@Test public void testTriggerJobSuccess() throws Exception { UnifiedAuditMessage auditMsg = new UnifiedAuditMessage(); auditMsg.setInputFilePath("src/test/resources/test.csv"); String fileType = ".csv";

when(kafkaAuditProducer.sendMessage(any(UnifiedAuditMessage.class))).thenReturn("Message sent successfully");

String result = flinkApiController.triggerJob(auditMsg, fileType);

assertEquals("Job submitted", result); verify(kafkaAuditProducer, times(1)).sendMessage(any(UnifiedAuditMessage.class)); } }